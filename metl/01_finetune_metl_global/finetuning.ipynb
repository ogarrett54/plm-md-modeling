{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30ea18e-6b5a-47d4-b7a4-1330804b5602",
   "metadata": {},
   "source": [
    "# Finetune on experimental data\n",
    "This notebook demonstrates how to finetune METL models on experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432eebaf-00b8-42bf-b927-fd651e6ab94d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T22:51:36.573559Z",
     "start_time": "2024-02-16T22:51:36.569490Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c566507e-1012-4415-82ba-7498950e0b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: /home/oscar/projects/strucbio_projects/plm_md_modeling/metl/metl/code\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Get the absolute path of the directory containing the current script.\n",
    "#    This will be your '01_finetune_metl_global' folder.\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 2. Get the parent directory of your current script's folder.\n",
    "#    This will be the top-level 'metl' directory that contains both of your folders.\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# 3. Construct the path to the sibling 'metl' repo's 'code' folder.\n",
    "module_path = os.path.join(parent_dir, \"metl\", \"code\")\n",
    "\n",
    "# 4. Add the module path to the system path so imports work.\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# You can add a print statement to confirm it's working correctly\n",
    "print(f\"Added to sys.path: {module_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19876208-66f9-46b5-8f50-8e798fa815a4",
   "metadata": {},
   "source": [
    "# Acquire an experimental dataset\n",
    "\n",
    "For demonstration purposes, this repository contains the [avGFP dataset](../data/dms_data/avgfp) from [Sarkisyan et al. (2016)](https://doi.org/10.1038/nature17995). \n",
    "See the [metl-pub](https://github.com/gitter-lab/metl-pub) repository to access the other experimental datasets we used in our preprint.\n",
    "See the README in the [dms_data](../data/dms_data) directory for information about how to use your own experimental dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6abf8b1-aa2d-4055-9184-d962ba0d4582",
   "metadata": {},
   "source": [
    "# Acquire a pretrained model\n",
    "Pretrained METL models are available in the [metl-pretrained](https://github.com/gitter-lab/metl-pretrained) repository. You can use one of those, or you can pretrain your own METL model (see [pretraining.ipynb](pretraining.ipynb)). \n",
    "\n",
    "For demonstration purposes, we include a pretrained avGFP METL-Local model from the [metl-pretrained](https://github.com/gitter-lab/metl-pretrained) repository in the [pretrained_models](../pretrained_models) directory. This model is `METL-L-2M-3D-GFP` (UUID: `Hr4GNHws`).\n",
    "It is the avGFP METL-Local source model we used for the analysis in our preprint.\n",
    "\n",
    "We will show how to finetune this model using the [experimental avGFP dataset](../data/dms_data/avgfp)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a30235-357a-4326-a4ff-77ab26eb5d7f",
   "metadata": {},
   "source": [
    "# Training arguments\n",
    "\n",
    "The script for finetuning on experimental data is [train_target_model.py](train_target_model.py). This script has a number of arguments you can view by uncommenting and running the below cell. There are additional arguments related to architecture that won't show up if you run the command, but you can view them in [models.py](../code/models.py) in the `TransferModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bca8aeea-3dc3-47eb-915c-d80132be8fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oscar/miniconda3/envs/metl/lib/python3.9/site-packages/pytorch_lightning/utilities/imports.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "usage: train_target_model.py [-h] [--seed SEED] [--early_stopping]\n",
      "                             [--es_monitor {train,val,auto}]\n",
      "                             [--es_patience ES_PATIENCE]\n",
      "                             [--es_min_delta ES_MIN_DELTA]\n",
      "                             [--ckpt_monitor {train,val,auto}] [--finetuning]\n",
      "                             [--finetuning_strategy {backbone,extract}]\n",
      "                             [--unfreeze_backbone_at_epoch UNFREEZE_BACKBONE_AT_EPOCH]\n",
      "                             [--train_bn] [--backbone_always_align_lr]\n",
      "                             [--backbone_initial_ratio_lr BACKBONE_INITIAL_RATIO_LR]\n",
      "                             [--backbone_initial_lr BACKBONE_INITIAL_LR]\n",
      "                             [--swa] [--swa_epoch_start SWA_EPOCH_START]\n",
      "                             [--swa_lr SWA_LR] [--save_last_metrics]\n",
      "                             [--enable_simple_progress_messages]\n",
      "                             [--cluster CLUSTER] [--process PROCESS]\n",
      "                             [--github_tag GITHUB_TAG]\n",
      "                             [--log_dir_base LOG_DIR_BASE] [--uuid UUID]\n",
      "                             [--use_wandb] [--no_use_wandb] [--wandb_online]\n",
      "                             [--wandb_project WANDB_PROJECT]\n",
      "                             [--experiment EXPERIMENT] [--wandb_log_grads]\n",
      "                             [--grad_log_freq GRAD_LOG_FREQ]\n",
      "                             [--delete_checkpoints] --ds_name DS_NAME\n",
      "                             [--pdb_fn PDB_FN] [--encoding ENCODING]\n",
      "                             [--target_names TARGET_NAMES] [--shuffle_targets]\n",
      "                             [--target_roll TARGET_ROLL]\n",
      "                             [--standardize_targets]\n",
      "                             [--target_offset TARGET_OFFSET]\n",
      "                             [--aux_input_names AUX_INPUT_NAMES [AUX_INPUT_NAMES ...]]\n",
      "                             [--aux-formats AUX_FORMATS]\n",
      "                             [--split_dir SPLIT_DIR] [--use_val_for_training]\n",
      "                             [--train_name TRAIN_NAME] [--val_name VAL_NAME]\n",
      "                             [--test_name TEST_NAME]\n",
      "                             [--predict_mode {all_sets,full_dataset,wt}]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--num_dataloader_workers NUM_DATALOADER_WORKERS]\n",
      "                             [--logger [LOGGER]]\n",
      "                             [--enable_checkpointing [ENABLE_CHECKPOINTING]]\n",
      "                             [--default_root_dir DEFAULT_ROOT_DIR]\n",
      "                             [--gradient_clip_val GRADIENT_CLIP_VAL]\n",
      "                             [--gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM]\n",
      "                             [--num_nodes NUM_NODES]\n",
      "                             [--num_processes NUM_PROCESSES]\n",
      "                             [--devices DEVICES] [--gpus GPUS]\n",
      "                             [--auto_select_gpus [AUTO_SELECT_GPUS]]\n",
      "                             [--tpu_cores TPU_CORES] [--ipus IPUS]\n",
      "                             [--enable_progress_bar [ENABLE_PROGRESS_BAR]]\n",
      "                             [--overfit_batches OVERFIT_BATCHES]\n",
      "                             [--track_grad_norm TRACK_GRAD_NORM]\n",
      "                             [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]\n",
      "                             [--fast_dev_run [FAST_DEV_RUN]]\n",
      "                             [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]\n",
      "                             [--max_epochs MAX_EPOCHS]\n",
      "                             [--min_epochs MIN_EPOCHS] [--max_steps MAX_STEPS]\n",
      "                             [--min_steps MIN_STEPS] [--max_time MAX_TIME]\n",
      "                             [--limit_train_batches LIMIT_TRAIN_BATCHES]\n",
      "                             [--limit_val_batches LIMIT_VAL_BATCHES]\n",
      "                             [--limit_test_batches LIMIT_TEST_BATCHES]\n",
      "                             [--limit_predict_batches LIMIT_PREDICT_BATCHES]\n",
      "                             [--val_check_interval VAL_CHECK_INTERVAL]\n",
      "                             [--log_every_n_steps LOG_EVERY_N_STEPS]\n",
      "                             [--accelerator ACCELERATOR] [--strategy STRATEGY]\n",
      "                             [--sync_batchnorm [SYNC_BATCHNORM]]\n",
      "                             [--precision PRECISION]\n",
      "                             [--enable_model_summary [ENABLE_MODEL_SUMMARY]]\n",
      "                             [--weights_save_path WEIGHTS_SAVE_PATH]\n",
      "                             [--num_sanity_val_steps NUM_SANITY_VAL_STEPS]\n",
      "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                             [--profiler PROFILER] [--benchmark [BENCHMARK]]\n",
      "                             [--deterministic [DETERMINISTIC]]\n",
      "                             [--reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS]\n",
      "                             [--auto_lr_find [AUTO_LR_FIND]]\n",
      "                             [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]]\n",
      "                             [--detect_anomaly [DETECT_ANOMALY]]\n",
      "                             [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]]\n",
      "                             [--plugins PLUGINS] [--amp_backend AMP_BACKEND]\n",
      "                             [--amp_level AMP_LEVEL]\n",
      "                             [--move_metrics_to_cpu [MOVE_METRICS_TO_CPU]]\n",
      "                             [--multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE]\n",
      "                             [--model_name {linear,fully_connected,cnn,cnn2,transformer_encoder,transfer_model}]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --seed SEED           random seed to use with pytorch lightning\n",
      "                        seed_everythingnot specifying a seed will use a random\n",
      "                        seed and record it to args for future runs\n",
      "  --early_stopping      set to enable early stopping\n",
      "  --es_monitor {train,val,auto}\n",
      "                        which loss to monitor\n",
      "  --es_patience ES_PATIENCE\n",
      "                        number of epochs allowance for early stopping\n",
      "  --es_min_delta ES_MIN_DELTA\n",
      "                        min by which the loss must decrease to be considered\n",
      "                        an improvement\n",
      "  --ckpt_monitor {train,val,auto}\n",
      "                        which loss to monitor for ckpt\n",
      "  --finetuning\n",
      "  --finetuning_strategy {backbone,extract}\n",
      "  --unfreeze_backbone_at_epoch UNFREEZE_BACKBONE_AT_EPOCH\n",
      "  --train_bn            whether to train batchnorm in backbone\n",
      "  --backbone_always_align_lr\n",
      "  --backbone_initial_ratio_lr BACKBONE_INITIAL_RATIO_LR\n",
      "  --backbone_initial_lr BACKBONE_INITIAL_LR\n",
      "  --swa                 set to enable stochastic weight averaging\n",
      "  --swa_epoch_start SWA_EPOCH_START\n",
      "  --swa_lr SWA_LR\n",
      "  --save_last_metrics   set to save metrics for the last checkpoint\n",
      "  --enable_simple_progress_messages\n",
      "                        set to enable simple progress messages\n",
      "  --cluster CLUSTER     cluster (when running on HTCondor)\n",
      "  --process PROCESS     process (when running on HTCondor)\n",
      "  --github_tag GITHUB_TAG\n",
      "                        github tag for current run\n",
      "  --log_dir_base LOG_DIR_BASE\n",
      "                        log directory base\n",
      "  --uuid UUID           model uuid to resume from or custom uuid to use from\n",
      "                        scratch\n",
      "  --use_wandb           use wandb for logging\n",
      "  --no_use_wandb\n",
      "  --wandb_online\n",
      "  --wandb_project WANDB_PROJECT\n",
      "  --experiment EXPERIMENT\n",
      "                        dummy arg to make wandb tracking and filtering easier\n",
      "  --wandb_log_grads     whether to log gradients and parameter histograms to\n",
      "                        weights&biases\n",
      "  --grad_log_freq GRAD_LOG_FREQ\n",
      "                        log frequency for gradients\n",
      "  --delete_checkpoints\n",
      "  --ds_name DS_NAME     name of the dms dataset defined in datasets.yml\n",
      "  --pdb_fn PDB_FN       pdb file for relative_3D position encoding\n",
      "  --encoding ENCODING   which data encoding to use\n",
      "  --target_names TARGET_NAMES\n",
      "                        the names of the target variables (currently only\n",
      "                        supports one target variable)\n",
      "  --shuffle_targets     whether to shuffle the target labels/scores, for\n",
      "                        debugging\n",
      "  --target_roll TARGET_ROLL\n",
      "                        how much to shift the targets relative to the\n",
      "                        variants, for debugging\n",
      "  --standardize_targets\n",
      "                        whether to standardize targets using train set\n",
      "  --target_offset TARGET_OFFSET\n",
      "                        an offset to add to every target value in the dataset\n",
      "  --aux_input_names AUX_INPUT_NAMES [AUX_INPUT_NAMES ...]\n",
      "                        the names of the auxiliary inputs which the model can\n",
      "                        access at any layer\n",
      "  --aux-formats AUX_FORMATS\n",
      "                        Comma-separated list of aux input formats, e.g.\n",
      "                        'feat1=tensor,feat2=numpy,feat3=string'. Valid formats\n",
      "                        are 'tensor', 'numpy', and 'string'. If not specified,\n",
      "                        all numerical aux inputs will be converted to tensors,\n",
      "                        and all string aux inputs will be left as strings.\n",
      "  --split_dir SPLIT_DIR\n",
      "                        the directory containing the train/tune/test split\n",
      "  --use_val_for_training\n",
      "                        whether to combine the val set with the train set for\n",
      "                        training\n",
      "  --train_name TRAIN_NAME\n",
      "                        name of the train set in the split dir\n",
      "  --val_name VAL_NAME   name of the validation set in the split dir\n",
      "  --test_name TEST_NAME\n",
      "                        name of the test set in the split dir\n",
      "  --predict_mode {all_sets,full_dataset,wt}\n",
      "                        what predict mode to use\n",
      "  --batch_size BATCH_SIZE\n",
      "                        batch size for the data loader and optimizer\n",
      "  --num_dataloader_workers NUM_DATALOADER_WORKERS\n",
      "                        number of workers for the data loader\n",
      "  --model_name {linear,fully_connected,cnn,cnn2,transformer_encoder,transfer_model}\n",
      "\n",
      "pl.Trainer:\n",
      "  --logger [LOGGER]     Logger (or iterable collection of loggers) for\n",
      "                        experiment tracking. A ``True`` value uses the default\n",
      "                        ``TensorBoardLogger``. ``False`` will disable logging.\n",
      "                        If multiple loggers are provided and the `save_dir`\n",
      "                        property of that logger is not set, local files\n",
      "                        (checkpoints, profiler traces, etc.) are saved in\n",
      "                        ``default_root_dir`` rather than in the ``log_dir`` of\n",
      "                        any of the individual loggers. Default: ``True``.\n",
      "  --enable_checkpointing [ENABLE_CHECKPOINTING]\n",
      "                        If ``True``, enable checkpointing. It will configure a\n",
      "                        default ModelCheckpoint callback if there is no user-\n",
      "                        defined ModelCheckpoint in :paramref:`~pytorch_lightni\n",
      "                        ng.trainer.trainer.Trainer.callbacks`. Default:\n",
      "                        ``True``.\n",
      "  --default_root_dir DEFAULT_ROOT_DIR\n",
      "                        Default path for logs and weights when no\n",
      "                        logger/ckpt_callback passed. Default: ``os.getcwd()``.\n",
      "                        Can be remote file paths such as `s3://mybucket/path`\n",
      "                        or 'hdfs://path/'\n",
      "  --gradient_clip_val GRADIENT_CLIP_VAL\n",
      "                        The value at which to clip gradients. Passing\n",
      "                        ``gradient_clip_val=None`` disables gradient clipping.\n",
      "                        If using Automatic Mixed Precision (AMP), the\n",
      "                        gradients will be unscaled before. Default: ``None``.\n",
      "  --gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM\n",
      "                        The gradient clipping algorithm to use. Pass\n",
      "                        ``gradient_clip_algorithm=\"value\"`` to clip by value,\n",
      "                        and ``gradient_clip_algorithm=\"norm\"`` to clip by\n",
      "                        norm. By default it will be set to ``\"norm\"``.\n",
      "  --num_nodes NUM_NODES\n",
      "                        Number of GPU nodes for distributed training. Default:\n",
      "                        ``1``.\n",
      "  --num_processes NUM_PROCESSES\n",
      "                        Number of processes for distributed training with\n",
      "                        ``accelerator=\"cpu\"``. Default: ``1``. .. deprecated::\n",
      "                        v1.7 ``num_processes`` has been deprecated in v1.7 and\n",
      "                        will be removed in v2.0. Please use\n",
      "                        ``accelerator='cpu'`` and ``devices=x`` instead.\n",
      "  --devices DEVICES     Will be mapped to either `gpus`, `tpu_cores`,\n",
      "                        `num_processes` or `ipus`, based on the accelerator\n",
      "                        type.\n",
      "  --gpus GPUS           Number of GPUs to train on (int) or which GPUs to\n",
      "                        train on (list or str) applied per node Default:\n",
      "                        ``None``. .. deprecated:: v1.7 ``gpus`` has been\n",
      "                        deprecated in v1.7 and will be removed in v2.0. Please\n",
      "                        use ``accelerator='gpu'`` and ``devices=x`` instead.\n",
      "  --auto_select_gpus [AUTO_SELECT_GPUS]\n",
      "                        If enabled and ``gpus`` or ``devices`` is an integer,\n",
      "                        pick available gpus automatically. This is especially\n",
      "                        useful when GPUs are configured to be in \"exclusive\n",
      "                        mode\", such that only one process at a time can access\n",
      "                        them. Default: ``False``.\n",
      "  --tpu_cores TPU_CORES\n",
      "                        How many TPU cores to train on (1 or 8) / Single TPU\n",
      "                        to train on (1) Default: ``None``. .. deprecated::\n",
      "                        v1.7 ``tpu_cores`` has been deprecated in v1.7 and\n",
      "                        will be removed in v2.0. Please use\n",
      "                        ``accelerator='tpu'`` and ``devices=x`` instead.\n",
      "  --ipus IPUS           How many IPUs to train on. Default: ``None``. ..\n",
      "                        deprecated:: v1.7 ``ipus`` has been deprecated in v1.7\n",
      "                        and will be removed in v2.0. Please use\n",
      "                        ``accelerator='ipu'`` and ``devices=x`` instead.\n",
      "  --enable_progress_bar [ENABLE_PROGRESS_BAR]\n",
      "                        Whether to enable to progress bar by default. Default:\n",
      "                        ``True``.\n",
      "  --overfit_batches OVERFIT_BATCHES\n",
      "                        Overfit a fraction of training/validation data (float)\n",
      "                        or a set number of batches (int). Default: ``0.0``.\n",
      "  --track_grad_norm TRACK_GRAD_NORM\n",
      "                        -1 no tracking. Otherwise tracks that p-norm. May be\n",
      "                        set to 'inf' infinity-norm. If using Automatic Mixed\n",
      "                        Precision (AMP), the gradients will be unscaled before\n",
      "                        logging them. Default: ``-1``.\n",
      "  --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH\n",
      "                        Perform a validation loop every after every `N`\n",
      "                        training epochs. If ``None``, validation will be done\n",
      "                        solely based on the number of training batches,\n",
      "                        requiring ``val_check_interval`` to be an integer\n",
      "                        value. Default: ``1``.\n",
      "  --fast_dev_run [FAST_DEV_RUN]\n",
      "                        Runs n if set to ``n`` (int) else 1 if set to ``True``\n",
      "                        batch(es) of train, val and test to find any bugs (ie:\n",
      "                        a sort of unit test). Default: ``False``.\n",
      "  --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES\n",
      "                        Accumulates grads every k batches or as set up in the\n",
      "                        dict. Default: ``None``.\n",
      "  --max_epochs MAX_EPOCHS\n",
      "                        Stop training once this number of epochs is reached.\n",
      "                        Disabled by default (None). If both max_epochs and\n",
      "                        max_steps are not specified, defaults to ``max_epochs\n",
      "                        = 1000``. To enable infinite training, set\n",
      "                        ``max_epochs = -1``.\n",
      "  --min_epochs MIN_EPOCHS\n",
      "                        Force training for at least these many epochs.\n",
      "                        Disabled by default (None).\n",
      "  --max_steps MAX_STEPS\n",
      "                        Stop training after this number of steps. Disabled by\n",
      "                        default (-1). If ``max_steps = -1`` and ``max_epochs =\n",
      "                        None``, will default to ``max_epochs = 1000``. To\n",
      "                        enable infinite training, set ``max_epochs`` to\n",
      "                        ``-1``.\n",
      "  --min_steps MIN_STEPS\n",
      "                        Force training for at least these number of steps.\n",
      "                        Disabled by default (``None``).\n",
      "  --max_time MAX_TIME   Stop training after this amount of time has passed.\n",
      "                        Disabled by default (``None``). The time duration can\n",
      "                        be specified in the format DD:HH:MM:SS (days, hours,\n",
      "                        minutes seconds), as a :class:`datetime.timedelta`, or\n",
      "                        a dictionary with keys that will be passed to\n",
      "                        :class:`datetime.timedelta`.\n",
      "  --limit_train_batches LIMIT_TRAIN_BATCHES\n",
      "                        How much of training dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``.\n",
      "  --limit_val_batches LIMIT_VAL_BATCHES\n",
      "                        How much of validation dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``.\n",
      "  --limit_test_batches LIMIT_TEST_BATCHES\n",
      "                        How much of test dataset to check (float = fraction,\n",
      "                        int = num_batches). Default: ``1.0``.\n",
      "  --limit_predict_batches LIMIT_PREDICT_BATCHES\n",
      "                        How much of prediction dataset to check (float =\n",
      "                        fraction, int = num_batches). Default: ``1.0``.\n",
      "  --val_check_interval VAL_CHECK_INTERVAL\n",
      "                        How often to check the validation set. Pass a\n",
      "                        ``float`` in the range [0.0, 1.0] to check after a\n",
      "                        fraction of the training epoch. Pass an ``int`` to\n",
      "                        check after a fixed number of training batches. An\n",
      "                        ``int`` value can only be higher than the number of\n",
      "                        training batches when\n",
      "                        ``check_val_every_n_epoch=None``, which validates\n",
      "                        after every ``N`` training batches across epochs or\n",
      "                        during iteration-based training. Default: ``1.0``.\n",
      "  --log_every_n_steps LOG_EVERY_N_STEPS\n",
      "                        How often to log within steps. Default: ``50``.\n",
      "  --accelerator ACCELERATOR\n",
      "                        Supports passing different accelerator types (\"cpu\",\n",
      "                        \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"mps, \"auto\") as well as\n",
      "                        custom accelerator instances. .. deprecated:: v1.5\n",
      "                        Passing training strategies (e.g., 'ddp') to\n",
      "                        ``accelerator`` has been deprecated in v1.5.0 and will\n",
      "                        be removed in v1.7.0. Please use the ``strategy``\n",
      "                        argument instead.\n",
      "  --strategy STRATEGY   Supports different training strategies with aliases as\n",
      "                        well custom strategies. Default: ``None``.\n",
      "  --sync_batchnorm [SYNC_BATCHNORM]\n",
      "                        Synchronize batch norm layers between process\n",
      "                        groups/whole world. Default: ``False``.\n",
      "  --precision PRECISION\n",
      "                        Double precision (64), full precision (32), half\n",
      "                        precision (16) or bfloat16 precision (bf16). Can be\n",
      "                        used on CPU, GPU, TPUs, HPUs or IPUs. Default: ``32``.\n",
      "  --enable_model_summary [ENABLE_MODEL_SUMMARY]\n",
      "                        Whether to enable model summarization by default.\n",
      "                        Default: ``True``.\n",
      "  --weights_save_path WEIGHTS_SAVE_PATH\n",
      "                        Where to save weights if specified. Will override\n",
      "                        default_root_dir for checkpoints only. Use this if for\n",
      "                        whatever reason you need the checkpoints stored in a\n",
      "                        different place than the logs written in\n",
      "                        `default_root_dir`. Can be remote file paths such as\n",
      "                        `s3://mybucket/path` or 'hdfs://path/' Defaults to\n",
      "                        `default_root_dir`. .. deprecated:: v1.6\n",
      "                        ``weights_save_path`` has been deprecated in v1.6 and\n",
      "                        will be removed in v1.8. Please pass ``dirpath``\n",
      "                        directly to the :class:`~pytorch_lightning.callbacks.m\n",
      "                        odel_checkpoint.ModelCheckpoint` callback.\n",
      "  --num_sanity_val_steps NUM_SANITY_VAL_STEPS\n",
      "                        Sanity check runs n validation batches before starting\n",
      "                        the training routine. Set it to `-1` to run all\n",
      "                        batches in all validation dataloaders. Default: ``2``.\n",
      "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
      "                        Path/URL of the checkpoint from which training is\n",
      "                        resumed. If there is no checkpoint file at the path,\n",
      "                        an exception is raised. If resuming from mid-epoch\n",
      "                        checkpoint, training will start from the beginning of\n",
      "                        the next epoch. .. deprecated:: v1.5\n",
      "                        ``resume_from_checkpoint`` is deprecated in v1.5 and\n",
      "                        will be removed in v2.0. Please pass the path to\n",
      "                        ``Trainer.fit(..., ckpt_path=...)`` instead.\n",
      "  --profiler PROFILER   To profile individual steps during training and assist\n",
      "                        in identifying bottlenecks. Default: ``None``.\n",
      "  --benchmark [BENCHMARK]\n",
      "                        The value (``True`` or ``False``) to set\n",
      "                        ``torch.backends.cudnn.benchmark`` to. The value for\n",
      "                        ``torch.backends.cudnn.benchmark`` set in the current\n",
      "                        session will be used (``False`` if not manually set).\n",
      "                        If :paramref:`~pytorch_lightning.trainer.Trainer.deter\n",
      "                        ministic` is set to ``True``, this will default to\n",
      "                        ``False``. Override to manually set a different value.\n",
      "                        Default: ``None``.\n",
      "  --deterministic [DETERMINISTIC]\n",
      "                        If ``True``, sets whether PyTorch operations must use\n",
      "                        deterministic algorithms. Set to ``\"warn\"`` to use\n",
      "                        deterministic algorithms whenever possible, throwing\n",
      "                        warnings on operations that don't support\n",
      "                        deterministic mode (requires PyTorch 1.11+). If not\n",
      "                        set, defaults to ``False``. Default: ``None``.\n",
      "  --reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS\n",
      "                        Set to a non-negative integer to reload dataloaders\n",
      "                        every n epochs. Default: ``0``.\n",
      "  --auto_lr_find [AUTO_LR_FIND]\n",
      "                        If set to True, will make trainer.tune() run a\n",
      "                        learning rate finder, trying to optimize initial\n",
      "                        learning for faster convergence. trainer.tune() method\n",
      "                        will set the suggested learning rate in self.lr or\n",
      "                        self.learning_rate in the LightningModule. To use a\n",
      "                        different key set a string instead of True with the\n",
      "                        key name. Default: ``False``.\n",
      "  --replace_sampler_ddp [REPLACE_SAMPLER_DDP]\n",
      "                        Explicitly enables or disables sampler replacement. If\n",
      "                        not specified this will toggled automatically when DDP\n",
      "                        is used. By default it will add ``shuffle=True`` for\n",
      "                        train sampler and ``shuffle=False`` for val/test\n",
      "                        sampler. If you want to customize it, you can set\n",
      "                        ``replace_sampler_ddp=False`` and add your own\n",
      "                        distributed sampler.\n",
      "  --detect_anomaly [DETECT_ANOMALY]\n",
      "                        Enable anomaly detection for the autograd engine.\n",
      "                        Default: ``False``.\n",
      "  --auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]\n",
      "                        If set to True, will `initially` run a batch size\n",
      "                        finder trying to find the largest batch size that fits\n",
      "                        into memory. The result will be stored in\n",
      "                        self.batch_size in the LightningModule. Additionally,\n",
      "                        can be set to either `power` that estimates the batch\n",
      "                        size through a power search or `binsearch` that\n",
      "                        estimates the batch size through a binary search.\n",
      "                        Default: ``False``.\n",
      "  --plugins PLUGINS     Plugins allow modification of core behavior like ddp\n",
      "                        and amp, and enable custom lightning plugins. Default:\n",
      "                        ``None``.\n",
      "  --amp_backend AMP_BACKEND\n",
      "                        The mixed precision backend to use (\"native\" or\n",
      "                        \"apex\"). Default: ``'native''``.\n",
      "  --amp_level AMP_LEVEL\n",
      "                        The optimization level to use (O1, O2, etc...). By\n",
      "                        default it will be set to \"O2\" if ``amp_backend`` is\n",
      "                        set to \"apex\".\n",
      "  --move_metrics_to_cpu [MOVE_METRICS_TO_CPU]\n",
      "                        Whether to force internal logged metrics to be moved\n",
      "                        to cpu. This can save some gpu memory, but can make\n",
      "                        training slower. Use with attention. Default:\n",
      "                        ``False``.\n",
      "  --multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE\n",
      "                        How to loop over the datasets when there are multiple\n",
      "                        train loaders. In 'max_size_cycle' mode, the trainer\n",
      "                        ends one epoch when the largest dataset is traversed,\n",
      "                        and smaller datasets reload when running out of their\n",
      "                        data. In 'min_size' mode, all the datasets reload when\n",
      "                        reaching the minimum length of datasets. Default:\n",
      "                        ``\"max_size_cycle\"``.\n"
     ]
    }
   ],
   "source": [
    "!python /home/oscar/projects/strucbio_projects/plm_md_modeling/metl/metl/code/train_target_model.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec8c31b-2da2-4ba7-9f4e-39e30dce8056",
   "metadata": {},
   "source": [
    "We set up finetuning arguments for this example in [finetune_avgfp_local.txt](../args/pretrain_avgfp_local.txt) in the [args](../args) directory. This argument file can be used directly with [train_target_model.py](train_target_model.py) by calling the command `!python code/train_target_model.py @args/finetune_avgfp_local.txt` (we do this in the next section).\n",
    "\n",
    "Uncomment and run the cell below to view the contents of the argument file. The sections below will walk through and explain the key arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a06a897f-877d-4e41-9bee-4d3eabeead7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"args/finetune_avgfp_local.txt\", \"r\") as file:\n",
    "#     contents = file.read()\n",
    "#     print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2610124-fa2c-4709-98fc-bae51b258338",
   "metadata": {},
   "source": [
    "## Dataset arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56ee90-90be-41fa-bc99-c13f94e14976",
   "metadata": {},
   "source": [
    "\n",
    "Specify the dataset name and the train/val/test split. The dataset must be defined in [datasets.yml](../data/dms_data/datasets.yml). For demonstration purposes, we are using one of the reduced dataset size splits with a dataset size of 160 (train size of 128).\n",
    "```\n",
    "--ds_name\n",
    "avgfp\n",
    "--split_dir\n",
    "data/dms_data/avgfp/splits/resampled/resampled_ds160_val0.2_te0.1_w1abc2f4e9a64_s1_r8099/resampled_ds160_val0.2_te0.1_w1abc2f4e9a64_s1_r8099_rep_0\n",
    "```\n",
    "\n",
    "Specify the names of the train, validation, and test set files in the split directory. Using \"auto\" for the test_name will select the super test set (\"stest.txt\") if it exists in the split directory, otherwise it will use the standard test set (\"test.txt\").\n",
    "\n",
    "```\n",
    "--train_name\n",
    "train\n",
    "--val_name\n",
    "val\n",
    "--test_name\n",
    "test\n",
    "```\n",
    "\n",
    "The name of the target column in the dataset dataframe. The model will be finetuned to predict the score in this column.\n",
    "\n",
    "```\n",
    "--target_names\n",
    "score\n",
    "```\n",
    "\n",
    "The METL-Local model we are finetuning uses 3D structure-based relative position embeddings, so we need to specify the PDB filename. This PDB file is in the [data/pdb_files](../data/pdb_files) directory, which the script checks by default, so there is no need to specify the full path. You can also just specify \"auto\" to use the PDB file defined for this dataset in [datasets.yml](../data/dms_data/datasets.yml).\n",
    "\n",
    "```\n",
    "--pdb_fn\n",
    "1gfl_cm.pdb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890cea13-feae-4e54-bf0f-dcbe97f4409f",
   "metadata": {},
   "source": [
    "## Network architecture arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee9762-cae7-4e21-8435-f6dd49781b8c",
   "metadata": {},
   "source": [
    "For finetuning, we implemented a special model `transfer_model` that handles pretrained checkpoints with top nets. \n",
    "```\n",
    "--model_name\n",
    "transfer_model\n",
    "```\n",
    "\n",
    "The pretrained checkpoint can be a PyTorch checkpoint (.pt file) downloaded from the [metl-pretrained](https://github.com/gitter-lab/metl-pretrained) repository or a PyTorch Lightning checkpoint (.ckpt file) obtained from pretraining a model with this repository.\n",
    "```\n",
    "--pretrained_ckpt_path\n",
    "pretrained_models/Hr4GNHws.pt\n",
    "```\n",
    "\n",
    "The backbone cutoff determines where to cutoff the pretrained model and place the new prediction head. For METL-Local models, we recommend backbone cutoff -1, and for METL-Global models we recommend backbone cutoff -2. \n",
    "\n",
    "```\n",
    "--backbone_cutoff\n",
    "-1\n",
    "```\n",
    "\n",
    "The remaining arguments determine the encoding, which should be set to `int_seqs`, whether to use dropout after the backbone cutoff, and the architecture of the new top net. You can leave these values as-is to match what we did for the preprint.\n",
    "\n",
    "```\n",
    "--encoding\n",
    "int_seqs\n",
    "--dropout_after_backbone\n",
    "--dropout_after_backbone_rate\n",
    "0.5\n",
    "--top_net_type\n",
    "linear\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d94c112-9770-4a5f-93e0-acf4d9acae16",
   "metadata": {},
   "source": [
    "## Finetuning strategy arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb96cb6-7815-4efa-9b6f-305df9bb3050",
   "metadata": {},
   "source": [
    "We implemented a dual-phase finetuning strategy. During the first phase, the backbone weights are frozen and only the top net is trained. During the second phase, all the network weights are unfrozen and trained at a reduced learning rate.\n",
    "\n",
    "The unfreeze_backbone_at_epoch argument determines the training epoch at which to unfreeze the backbone. We train the models for 500 epochs, so the backbone is unfrozen halfway through at epoch 250.\n",
    "\n",
    "```\n",
    "--finetuning\n",
    "--finetuning_strategy\n",
    "backbone\n",
    "--unfreeze_backbone_at_epoch\n",
    "250\n",
    "--backbone_always_align_lr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d4584-a0ce-45c3-8fb7-8c34d3a984c3",
   "metadata": {},
   "source": [
    "## Optimization arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d10e8-21f3-4b9e-8134-99cb053bef13",
   "metadata": {},
   "source": [
    "Basic optimizer arguments include the batch size, learning rate, and maximum number of epochs to train for. Unless early stopping is enabled, the model will train for the given number of epochs. \n",
    "\n",
    "```\n",
    "--optimizer\n",
    "adamw\n",
    "--weight_decay\n",
    "0.1\n",
    "--batch_size\n",
    "128\n",
    "--learning_rate\n",
    "0.001\n",
    "--max_epochs\n",
    "500\n",
    "--gradient_clip_val\n",
    "0.5\n",
    "```\n",
    "\n",
    "The learning rate scheduler we used for finetuning is a dual phase learning rate schedule that matches the dual phase finetuning strategy. Each phase has a linear learning rate warmup for 1% of the total steps in that phase. There is also a cosine decay for the learning rate for each phase. The phase 2 learning rate is 10% of the phase 1 learning rate.\n",
    "\n",
    "```\n",
    "--lr_scheduler\n",
    "dual_phase_warmup_constant_cosine_decay\n",
    "--warmup_steps\n",
    ".01\n",
    "--phase2_lr_ratio\n",
    "0.1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16327f53-7beb-412e-a925-12884e66d70b",
   "metadata": {},
   "source": [
    "## Logging arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132db93c-85e6-4658-a31e-9b103df34cb7",
   "metadata": {},
   "source": [
    "We have built in functionality for tracking model training with Weights & Biases. If you have a Weights and Biases account, set the environment variable `WANDB_API_KEY` to your API key and set the flag `--use_wandb` instead of `--no_use_wandb` below.\n",
    "\n",
    "```\n",
    "--no_use_wandb\n",
    "--wandb_project\n",
    "metl-target\n",
    "--wandb_online\n",
    "--experiment\n",
    "default\n",
    "```\n",
    "\n",
    "The below argument determines where to place the log directory locally.\n",
    "```\n",
    "--log_dir_base\n",
    "output/training_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2fda3-6dfa-46d5-ad3d-3055eda0b29a",
   "metadata": {},
   "source": [
    "# Running training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d8d23-9d54-4888-842d-4fc8fd843b40",
   "metadata": {},
   "source": [
    "All the arguments described above are contained in [finetune_avgfp_local.txt](../args/pretrain_avgfp_local.txt), which can be fed directly into [train_target_model.py](train_target_model.py).\n",
    "\n",
    "PyTorch Lightning has a built-in progress bar that is convenient for seeing training progress, but it does not display correctly in Jupyter when calling the script with `!python`. We are going to disable the progress bar for by setting the flag `--enable_progress_bar false`. Instead, we implemented a simple print statement to track training progress, which we will enable with the flag `--enable_simple_progress_messages`. \n",
    "\n",
    "The [train_target_model.py](../code/train_target_model.py) script can support running on Apple Silicon with acceleration via MPS, but the version of PyTorch used in this environment is slightly outdated and does not support all MPS operations, so MPS support has been disabled. The script will run on GPU via CUDA if available, otherwise it will use CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be213e48",
   "metadata": {},
   "source": [
    "Experiment keys:\n",
    "- dX = X data points\n",
    "- baseline = the reference experiment\n",
    "- exp = experiment\n",
    "- bsX = batch size changed to X\n",
    "- lrrX = phase2 lr ratio of 1/X\n",
    "- espX = early stopping patience set to \n",
    "- wdX = weight decay of 1/X\n",
    "- lrX = learning rate of 1/X\n",
    "\n",
    "List of experiments:\n",
    "- d320\n",
    "    - bs8\n",
    "    - bs16\n",
    "    - bs16 lrr5 esp25\n",
    "    - bs16 lrr100\n",
    "    - bs16 lrr100 esp25\n",
    "    - bs32 esp25 wd100 lr100\n",
    "    - bs32 lrr5 esp25\n",
    "    - bs32 lrr5 esp25 wd5\n",
    "    - bs32 lrr5 esp25 wd100\n",
    "    - bs32 lrr5 esp25 wd100 lr100\n",
    "    - bs32 lrr5 esp25 wd100 lr10000\n",
    "    - bs64\n",
    "- d1280\n",
    "    - bs32 esp25 wd100\n",
    "    - bs32 lrr5 esp25 wd100"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "f4b952ae",
   "metadata": {},
   "source": [
    "Previous best: d320_exp_bs32_lrr5_esp25_wd100\n",
    "- ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98a30217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oscar/miniconda3/envs/metl/lib/python3.9/site-packages/pytorch_lightning/utilities/imports.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "Random seed not specified, using: 209772873\n",
      "Global seed set to 209772873\n",
      "User gave model UUID: d1280_exp_bs32_esp25_wd100\n",
      "Did not find existing log directory corresponding to given UUID: d1280_exp_bs32_esp25_wd100\n",
      "Created log directory: output/training_logs/d1280_exp_bs32_esp25_wd100\n",
      "Final UUID: d1280_exp_bs32_esp25_wd100\n",
      "Final log directory: output/training_logs/d1280_exp_bs32_esp25_wd100\n",
      "/home/oscar/projects/strucbio_projects/plm_md_modeling/metl/metl/code/train_target_model.py:136: UserWarning: Using epoch-based backbone finetuning with early stopping enabled. It is possible early stopping triggers before the backbone is unfrozen, thus no finetuning would take place. Consider using in combination with min_epochs so early stopping only triggers during the finetuning phase, if that's what you're going for.\n",
      "  warnings.warn(\"Using epoch-based backbone finetuning with early stopping enabled. It is possible early \"\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/oscar/miniconda3/envs/metl/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (32) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Number of training steps is 3200\n",
      "Number of warmup steps is 32.0\n",
      "Second warmup phase starts at step 1600\n",
      "total_steps 3200\n",
      "phase1_total_steps 1600\n",
      "phase2_total_steps 1600\n",
      "┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
      "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType          \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m In sizes\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mOut sizes\u001b[0m\u001b[1;35m \u001b[0m┃\n",
      "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
      "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model                  │ TransferModel  │ 18.9 M │\u001b[37m \u001b[0m\u001b[37m [32, 78]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m  [32, 1]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ model.model            │ SequentialWit… │ 18.9 M │\u001b[37m \u001b[0m\u001b[37m [32, 78]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m  [32, 1]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ model.model.backbone   │ SequentialWit… │ 18.9 M │\u001b[37m \u001b[0m\u001b[37m [32, 78]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m[32, 512]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ model.model.dropout    │ Dropout        │      0 │\u001b[37m \u001b[0m\u001b[37m[32, 512]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m[32, 512]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ model.model.flatten    │ Flatten        │      0 │\u001b[37m \u001b[0m\u001b[37m[32, 512]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m[32, 512]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ model.model.prediction │ Linear         │    513 │\u001b[37m \u001b[0m\u001b[37m[32, 512]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m  [32, 1]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ test_pearson           │ PearsonCorrCo… │      0 │\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m7\u001b[0m\u001b[2m \u001b[0m│ test_spearman          │ SpearmanCorrC… │      0 │\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m        ?\u001b[0m\u001b[37m \u001b[0m│\n",
      "└───┴────────────────────────┴────────────────┴────────┴───────────┴───────────┘\n",
      "\u001b[1mTrainable params\u001b[0m: 513                                                           \n",
      "\u001b[1mNon-trainable params\u001b[0m: 18.9 M                                                    \n",
      "\u001b[1mTotal params\u001b[0m: 18.9 M                                                            \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 75                                      \n",
      "Sanity Checking: 0it [00:00, ?it/s]Starting sanity check...\n",
      "Sanity check complete.                                                          \n",
      "Training: 0it [00:00, ?it/s]Starting training...\n",
      "Epoch 0:  75%|██████████▌   | 30/40 [00:00<00:00, 46.37it/s, loss=0.224, v_num=]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 40/40 [00:00<00:00, 48.29it/s, loss=0.222, v_num=, val_loss=0.1\u001b[A\n",
      "Epoch 0: 100%|█| 40/40 [00:00<00:00, 48.22it/s, loss=0.222, v_num=, val_loss=0.1Epoch     0: Train Loss =   0.225, Val Loss =   0.173\n",
      "Metric val_loss improved. New best score: 0.173\n",
      "Epoch 1:  75%|▊| 30/40 [00:00<00:00, 55.30it/s, loss=0.213, v_num=, val_loss=0.1\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█| 40/40 [00:00<00:00, 55.60it/s, loss=0.212, v_num=, val_loss=0.1\u001b[A\n",
      "Epoch 1: 100%|█| 40/40 [00:00<00:00, 55.52it/s, loss=0.212, v_num=, val_loss=0.1Epoch     1: Train Loss =   0.208, Val Loss =   0.162\n",
      "Metric val_loss improved by 0.010 >= min_delta = 0.001. New best score: 0.162\n",
      "Epoch 2:  75%|▊| 30/40 [00:00<00:00, 57.00it/s, loss=0.208, v_num=, val_loss=0.1\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█| 40/40 [00:00<00:00, 56.97it/s, loss=0.21, v_num=, val_loss=0.15\u001b[A\n",
      "Epoch 2: 100%|█| 40/40 [00:00<00:00, 56.89it/s, loss=0.21, v_num=, val_loss=0.15Epoch     2: Train Loss =   0.203, Val Loss =   0.155\n",
      "Metric val_loss improved by 0.007 >= min_delta = 0.001. New best score: 0.155\n",
      "Epoch 3:  75%|▊| 30/40 [00:00<00:00, 54.30it/s, loss=0.195, v_num=, val_loss=0.1\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█| 40/40 [00:00<00:00, 54.72it/s, loss=0.192, v_num=, val_loss=0.1\u001b[A\n",
      "Epoch 3: 100%|█| 40/40 [00:00<00:00, 54.65it/s, loss=0.192, v_num=, val_loss=0.1Epoch     3: Train Loss =   0.190, Val Loss =   0.149\n",
      "Metric val_loss improved by 0.006 >= min_delta = 0.001. New best score: 0.149\n",
      "Epoch 4:  75%|▊| 30/40 [00:00<00:00, 55.62it/s, loss=0.197, v_num=, val_loss=0.1\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█| 40/40 [00:00<00:00, 55.66it/s, loss=0.194, v_num=, val_loss=0.1\u001b[A\n",
      "Epoch 4: 100%|█| 40/40 [00:00<00:00, 55.57it/s, loss=0.194, v_num=, val_loss=0.1Epoch     4: Train Loss =   0.192, Val Loss =   0.144\n",
      "Metric val_loss improved by 0.005 >= min_delta = 0.001. New best score: 0.144\n",
      "Epoch 5:  75%|▊| 30/40 [00:00<00:00, 57.25it/s, loss=0.19, v_num=, val_loss=0.14\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█| 40/40 [00:00<00:00, 56.99it/s, loss=0.19, v_num=, val_loss=0.14\u001b[A\n",
      "Epoch 5: 100%|█| 40/40 [00:00<00:00, 56.91it/s, loss=0.19, v_num=, val_loss=0.14Epoch     5: Train Loss =   0.183, Val Loss =   0.140\n",
      "Metric val_loss improved by 0.005 >= min_delta = 0.001. New best score: 0.140\n",
      "Epoch 6:  75%|▊| 30/40 [00:00<00:00, 53.62it/s, loss=0.182, v_num=, val_loss=0.1\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█| 40/40 [00:00<00:00, 54.05it/s, loss=0.182, v_num=, val_loss=0.1\u001b[A\n",
      "Epoch 6: 100%|█| 40/40 [00:00<00:00, 53.97it/s, loss=0.182, v_num=, val_loss=0.1Epoch     6: Train Loss =   0.180, Val Loss =   0.136\n",
      "Metric val_loss improved by 0.004 >= min_delta = 0.001. New best score: 0.136\n",
      "Epoch 7:  75%|▊| 30/40 [00:00<00:00, 55.42it/s, loss=0.185, v_num=, val_loss=0.1\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█| 40/40 [00:00<00:00, 55.84it/s, loss=0.185, v_num=, val_loss=0.1\u001b[A\n",
      "Epoch 7: 100%|█| 40/40 [00:00<00:00, 55.77it/s, loss=0.185, v_num=, val_loss=0.1Epoch     7: Train Loss =   0.178, Val Loss =   0.132\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.001. New best score: 0.132\n",
      "Epoch 8:  75%|▊| 30/40 [00:00<00:00, 57.23it/s, loss=0.185, v_num=, val_loss=0.1\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█| 40/40 [00:00<00:00, 56.90it/s, loss=0.183, v_num=, val_loss=0.1\u001b[A\n",
      "Epoch 8: 100%|█| 40/40 [00:00<00:00, 56.80it/s, loss=0.183, v_num=, val_loss=0.1Epoch     8: Train Loss =   0.176, Val Loss =   0.130\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.001. New best score: 0.130\n",
      "Epoch 9:  75%|▊| 30/40 [00:00<00:00, 54.43it/s, loss=0.18, v_num=, val_loss=0.13\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 40/40 [00:00<00:00, 54.86it/s, loss=0.176, v_num=, val_loss=0.1\u001b[A\n",
      "Epoch 9: 100%|█| 40/40 [00:00<00:00, 54.77it/s, loss=0.176, v_num=, val_loss=0.1Epoch     9: Train Loss =   0.173, Val Loss =   0.127\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 0.127\n",
      "Epoch 10:  75%|▊| 30/40 [00:00<00:00, 56.63it/s, loss=0.171, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 40/40 [00:00<00:00, 55.80it/s, loss=0.174, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 10: 100%|█| 40/40 [00:00<00:00, 55.72it/s, loss=0.174, v_num=, val_loss=0.Epoch    10: Train Loss =   0.168, Val Loss =   0.126\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 0.126\n",
      "Epoch 11:  75%|▊| 30/40 [00:00<00:00, 56.18it/s, loss=0.169, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 40/40 [00:00<00:00, 55.89it/s, loss=0.169, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 11: 100%|█| 40/40 [00:00<00:00, 55.75it/s, loss=0.169, v_num=, val_loss=0.Epoch    11: Train Loss =   0.164, Val Loss =   0.124\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.001. New best score: 0.124\n",
      "Epoch 12:  75%|▊| 30/40 [00:00<00:00, 54.75it/s, loss=0.176, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 40/40 [00:00<00:00, 55.01it/s, loss=0.175, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 12: 100%|█| 40/40 [00:00<00:00, 54.94it/s, loss=0.175, v_num=, val_loss=0.Epoch    12: Train Loss =   0.169, Val Loss =   0.122\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 0.122\n",
      "Epoch 13:  75%|▊| 30/40 [00:00<00:00, 56.81it/s, loss=0.176, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 40/40 [00:00<00:00, 56.76it/s, loss=0.175, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 13: 100%|█| 40/40 [00:00<00:00, 56.68it/s, loss=0.175, v_num=, val_loss=0.Epoch    13: Train Loss =   0.167, Val Loss =   0.122\n",
      "Epoch 14:  75%|▊| 30/40 [00:00<00:00, 56.55it/s, loss=0.166, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 40/40 [00:00<00:00, 56.52it/s, loss=0.164, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 14: 100%|█| 40/40 [00:00<00:00, 56.44it/s, loss=0.164, v_num=, val_loss=0.Epoch    14: Train Loss =   0.161, Val Loss =   0.120\n",
      "Metric val_loss improved by 0.003 >= min_delta = 0.001. New best score: 0.120\n",
      "Epoch 15:  75%|▊| 30/40 [00:00<00:00, 55.43it/s, loss=0.168, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 40/40 [00:00<00:00, 55.25it/s, loss=0.166, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 15: 100%|█| 40/40 [00:00<00:00, 55.10it/s, loss=0.166, v_num=, val_loss=0.Epoch    15: Train Loss =   0.163, Val Loss =   0.119\n",
      "Epoch 16:  75%|▊| 30/40 [00:00<00:00, 56.55it/s, loss=0.168, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 40/40 [00:00<00:00, 56.37it/s, loss=0.166, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 16: 100%|█| 40/40 [00:00<00:00, 56.29it/s, loss=0.166, v_num=, val_loss=0.Epoch    16: Train Loss =   0.167, Val Loss =   0.118\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 0.118\n",
      "Epoch 17:  75%|▊| 30/40 [00:00<00:00, 56.04it/s, loss=0.168, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 40/40 [00:00<00:00, 56.27it/s, loss=0.166, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 17: 100%|█| 40/40 [00:00<00:00, 56.19it/s, loss=0.166, v_num=, val_loss=0.Epoch    17: Train Loss =   0.162, Val Loss =   0.118\n",
      "Epoch 18:  75%|▊| 30/40 [00:00<00:00, 54.28it/s, loss=0.164, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 40/40 [00:00<00:00, 54.71it/s, loss=0.162, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 18: 100%|█| 40/40 [00:00<00:00, 54.57it/s, loss=0.162, v_num=, val_loss=0.Epoch    18: Train Loss =   0.160, Val Loss =   0.117\n",
      "Epoch 19:  75%|▊| 30/40 [00:00<00:00, 56.95it/s, loss=0.162, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 40/40 [00:00<00:00, 56.70it/s, loss=0.159, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 19: 100%|█| 40/40 [00:00<00:00, 56.61it/s, loss=0.159, v_num=, val_loss=0.Epoch    19: Train Loss =   0.157, Val Loss =   0.116\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 0.116\n",
      "Epoch 20:  75%|▊| 30/40 [00:00<00:00, 55.81it/s, loss=0.168, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 40/40 [00:00<00:00, 55.89it/s, loss=0.168, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 20: 100%|█| 40/40 [00:00<00:00, 55.81it/s, loss=0.168, v_num=, val_loss=0.Epoch    20: Train Loss =   0.162, Val Loss =   0.116\n",
      "Epoch 21:  75%|▊| 30/40 [00:00<00:00, 54.66it/s, loss=0.165, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 40/40 [00:00<00:00, 54.87it/s, loss=0.165, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 21: 100%|█| 40/40 [00:00<00:00, 54.80it/s, loss=0.165, v_num=, val_loss=0.Epoch    21: Train Loss =   0.158, Val Loss =   0.116\n",
      "Epoch 22:  75%|▊| 30/40 [00:00<00:00, -38.68it/s, loss=0.165, v_num=, val_loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 40/40 [00:00<00:00, -67.33it/s, loss=0.16, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 22: 100%|█| 40/40 [00:00<00:00, -67.47it/s, loss=0.16, v_num=, val_loss=0.Epoch    22: Train Loss =   0.159, Val Loss =   0.114\n",
      "Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 0.114\n",
      "Epoch 23:  75%|▊| 30/40 [00:00<00:00, 56.15it/s, loss=0.168, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|█| 40/40 [00:00<00:00, 55.76it/s, loss=0.165, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 23: 100%|█| 40/40 [00:00<00:00, 55.68it/s, loss=0.165, v_num=, val_loss=0.Epoch    23: Train Loss =   0.161, Val Loss =   0.114\n",
      "Epoch 24:  75%|▊| 30/40 [00:00<00:00, 56.86it/s, loss=0.16, v_num=, val_loss=0.1\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|█| 40/40 [00:00<00:00, 52.93it/s, loss=0.158, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 24: 100%|█| 40/40 [00:00<00:00, 52.85it/s, loss=0.158, v_num=, val_loss=0.Epoch    24: Train Loss =   0.155, Val Loss =   0.113\n",
      "Epoch 25:  75%|▊| 30/40 [00:00<00:00, 56.26it/s, loss=0.168, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|█| 40/40 [00:00<00:00, 56.29it/s, loss=0.164, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 25: 100%|█| 40/40 [00:00<00:00, 56.20it/s, loss=0.164, v_num=, val_loss=0.\u001b[AEpoch    25: Train Loss =   0.161, Val Loss =   0.113\n",
      "Epoch 26:  75%|▊| 30/40 [00:00<00:00, 55.80it/s, loss=0.168, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|█| 40/40 [00:00<00:00, 55.68it/s, loss=0.166, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 26: 100%|█| 40/40 [00:00<00:00, 55.60it/s, loss=0.166, v_num=, val_loss=0.Epoch    26: Train Loss =   0.161, Val Loss =   0.113\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 0.113\n",
      "Epoch 27:  75%|▊| 30/40 [00:00<00:00, 56.55it/s, loss=0.162, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|█| 40/40 [00:00<00:00, 56.09it/s, loss=0.161, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 27: 100%|█| 40/40 [00:00<00:00, 56.00it/s, loss=0.161, v_num=, val_loss=0.Epoch    27: Train Loss =   0.156, Val Loss =   0.114\n",
      "Epoch 28:  75%|▊| 30/40 [00:00<00:00, 54.68it/s, loss=0.17, v_num=, val_loss=0.1\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|█| 40/40 [00:00<00:00, 54.95it/s, loss=0.168, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 28: 100%|█| 40/40 [00:00<00:00, 54.85it/s, loss=0.168, v_num=, val_loss=0.Epoch    28: Train Loss =   0.162, Val Loss =   0.113\n",
      "Epoch 29:  75%|▊| 30/40 [00:00<00:00, 55.17it/s, loss=0.164, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 40/40 [00:00<00:00, 55.40it/s, loss=0.162, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 29: 100%|█| 40/40 [00:00<00:00, 55.32it/s, loss=0.162, v_num=, val_loss=0.Epoch    29: Train Loss =   0.157, Val Loss =   0.112\n",
      "Epoch 30:  75%|▊| 30/40 [00:00<00:00, 57.54it/s, loss=0.168, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30: 100%|█| 40/40 [00:00<00:00, 57.27it/s, loss=0.166, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 30: 100%|█| 40/40 [00:00<00:00, 57.19it/s, loss=0.166, v_num=, val_loss=0.Epoch    30: Train Loss =   0.161, Val Loss =   0.114\n",
      "Epoch 31:  75%|▊| 30/40 [00:00<00:00, 53.01it/s, loss=0.154, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31: 100%|█| 40/40 [00:00<00:00, 53.61it/s, loss=0.154, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 31: 100%|█| 40/40 [00:00<00:00, 53.51it/s, loss=0.154, v_num=, val_loss=0.Epoch    31: Train Loss =   0.152, Val Loss =   0.112\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 0.112\n",
      "Epoch 32:  75%|▊| 30/40 [00:00<00:00, 55.70it/s, loss=0.16, v_num=, val_loss=0.1\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32: 100%|█| 40/40 [00:00<00:00, 55.69it/s, loss=0.159, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 32: 100%|█| 40/40 [00:00<00:00, 55.60it/s, loss=0.159, v_num=, val_loss=0.Epoch    32: Train Loss =   0.155, Val Loss =   0.113\n",
      "Epoch 33:  75%|▊| 30/40 [00:00<00:00, 56.65it/s, loss=0.166, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33: 100%|█| 40/40 [00:00<00:00, 56.55it/s, loss=0.164, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 33: 100%|█| 40/40 [00:00<00:00, 56.45it/s, loss=0.164, v_num=, val_loss=0.Epoch    33: Train Loss =   0.158, Val Loss =   0.112\n",
      "Epoch 34:  75%|▊| 30/40 [00:00<00:00, 54.00it/s, loss=0.162, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34: 100%|█| 40/40 [00:00<00:00, 54.42it/s, loss=0.16, v_num=, val_loss=0.1\u001b[A\n",
      "Epoch 34: 100%|█| 40/40 [00:00<00:00, 54.32it/s, loss=0.16, v_num=, val_loss=0.1Epoch    34: Train Loss =   0.158, Val Loss =   0.113\n",
      "Epoch 35:  75%|▊| 30/40 [00:00<00:00, 56.44it/s, loss=0.158, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 35: 100%|█| 40/40 [00:00<00:00, 55.50it/s, loss=0.158, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 35: 100%|█| 40/40 [00:00<00:00, 55.42it/s, loss=0.158, v_num=, val_loss=0.Epoch    35: Train Loss =   0.155, Val Loss =   0.112\n",
      "Epoch 36:  75%|▊| 30/40 [00:00<00:00, 57.08it/s, loss=0.166, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36: 100%|█| 40/40 [00:00<00:00, 56.49it/s, loss=0.161, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 36: 100%|█| 40/40 [00:00<00:00, 56.41it/s, loss=0.161, v_num=, val_loss=0.Epoch    36: Train Loss =   0.157, Val Loss =   0.112\n",
      "Epoch 37:  75%|▊| 30/40 [00:00<00:00, 53.83it/s, loss=0.157, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 37: 100%|█| 40/40 [00:00<00:00, 54.47it/s, loss=0.158, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 37: 100%|█| 40/40 [00:00<00:00, 54.39it/s, loss=0.158, v_num=, val_loss=0.Epoch    37: Train Loss =   0.155, Val Loss =   0.113\n",
      "Epoch 38:  75%|▊| 30/40 [00:00<00:00, 56.17it/s, loss=0.16, v_num=, val_loss=0.1\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 38: 100%|█| 40/40 [00:00<00:00, 56.00it/s, loss=0.161, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 38: 100%|█| 40/40 [00:00<00:00, 55.92it/s, loss=0.161, v_num=, val_loss=0.Epoch    38: Train Loss =   0.158, Val Loss =   0.112\n",
      "Epoch 39:  75%|▊| 30/40 [00:00<00:00, 56.29it/s, loss=0.159, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 40/40 [00:00<00:00, 56.18it/s, loss=0.156, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 39: 100%|█| 40/40 [00:00<00:00, 56.10it/s, loss=0.156, v_num=, val_loss=0.Epoch    39: Train Loss =   0.154, Val Loss =   0.112\n",
      "Epoch 40:  75%|▊| 30/40 [00:00<00:00, 54.24it/s, loss=0.163, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 40: 100%|█| 40/40 [00:00<00:00, 54.46it/s, loss=0.161, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 40: 100%|█| 40/40 [00:00<00:00, 54.38it/s, loss=0.161, v_num=, val_loss=0.Epoch    40: Train Loss =   0.157, Val Loss =   0.112\n",
      "Epoch 41:  75%|▊| 30/40 [00:00<00:00, 56.09it/s, loss=0.159, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 41: 100%|█| 40/40 [00:00<00:00, 55.70it/s, loss=0.158, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 41: 100%|█| 40/40 [00:00<00:00, 55.62it/s, loss=0.158, v_num=, val_loss=0.Epoch    41: Train Loss =   0.154, Val Loss =   0.112\n",
      "Epoch 42:  75%|▊| 30/40 [00:00<00:00, 55.33it/s, loss=0.158, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 42: 100%|█| 40/40 [00:00<00:00, 55.26it/s, loss=0.157, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 42: 100%|█| 40/40 [00:00<00:00, 55.18it/s, loss=0.157, v_num=, val_loss=0.Epoch    42: Train Loss =   0.154, Val Loss =   0.112\n",
      "Epoch 43:  75%|▊| 30/40 [00:00<00:00, 52.73it/s, loss=0.159, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 43: 100%|█| 40/40 [00:00<00:00, 53.10it/s, loss=0.157, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 43: 100%|█| 40/40 [00:00<00:00, 52.98it/s, loss=0.157, v_num=, val_loss=0.Epoch    43: Train Loss =   0.154, Val Loss =   0.112\n",
      "Epoch 44:  75%|▊| 30/40 [00:00<00:00, 56.69it/s, loss=0.162, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 44: 100%|█| 40/40 [00:00<00:00, 56.46it/s, loss=0.161, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 44: 100%|█| 40/40 [00:00<00:00, 56.37it/s, loss=0.161, v_num=, val_loss=0.Epoch    44: Train Loss =   0.158, Val Loss =   0.112\n",
      "Epoch 45:  75%|▊| 30/40 [00:00<00:00, 55.48it/s, loss=0.155, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45: 100%|█| 40/40 [00:00<00:00, 55.50it/s, loss=0.157, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 45: 100%|█| 40/40 [00:00<00:00, 55.41it/s, loss=0.157, v_num=, val_loss=0.Epoch    45: Train Loss =   0.152, Val Loss =   0.112\n",
      "Epoch 46:  75%|▊| 30/40 [00:00<00:00, 53.99it/s, loss=0.157, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46: 100%|█| 40/40 [00:00<00:00, 54.38it/s, loss=0.156, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 46: 100%|█| 40/40 [00:00<00:00, 54.29it/s, loss=0.156, v_num=, val_loss=0.Epoch    46: Train Loss =   0.155, Val Loss =   0.112\n",
      "Epoch 47:  75%|▊| 30/40 [00:00<00:00, 57.17it/s, loss=0.167, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 47: 100%|█| 40/40 [00:00<00:00, 56.50it/s, loss=0.169, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 47: 100%|█| 40/40 [00:00<00:00, 56.42it/s, loss=0.169, v_num=, val_loss=0.Epoch    47: Train Loss =   0.159, Val Loss =   0.112\n",
      "Epoch 48:  75%|▊| 30/40 [00:00<00:00, 55.48it/s, loss=0.162, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 48: 100%|█| 40/40 [00:00<00:00, 55.40it/s, loss=0.161, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 48: 100%|█| 40/40 [00:00<00:00, 55.32it/s, loss=0.161, v_num=, val_loss=0.Epoch    48: Train Loss =   0.157, Val Loss =   0.112\n",
      "Epoch 49:  75%|▊| 30/40 [00:00<00:00, 56.04it/s, loss=0.161, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 40/40 [00:00<00:00, 52.47it/s, loss=0.161, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 49: 100%|█| 40/40 [00:00<00:00, 52.38it/s, loss=0.161, v_num=, val_loss=0.Epoch    49: Train Loss =   0.156, Val Loss =   0.112\n",
      "Epoch 50:  75%|▊| 30/40 [00:02<00:00, 14.62it/s, loss=0.158, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 50: 100%|█| 40/40 [00:02<00:00, 17.12it/s, loss=0.155, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 50: 100%|█| 40/40 [00:02<00:00, 17.11it/s, loss=0.155, v_num=, val_loss=0.Epoch    50: Train Loss =   0.153, Val Loss =   0.112\n",
      "Epoch 51:  75%|▊| 30/40 [00:02<00:00, 14.35it/s, loss=0.163, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 51: 100%|█| 40/40 [00:02<00:00, 16.82it/s, loss=0.164, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 51: 100%|█| 40/40 [00:02<00:00, 16.81it/s, loss=0.164, v_num=, val_loss=0.Epoch    51: Train Loss =   0.158, Val Loss =   0.102\n",
      "Metric val_loss improved by 0.010 >= min_delta = 0.001. New best score: 0.102\n",
      "Epoch 52:  75%|▊| 30/40 [00:02<00:00, 14.18it/s, loss=0.142, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 52: 100%|█| 40/40 [00:02<00:00, 16.64it/s, loss=0.141, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 52: 100%|█| 40/40 [00:02<00:00, 16.64it/s, loss=0.141, v_num=, val_loss=0.Epoch    52: Train Loss =   0.139, Val Loss =   0.112\n",
      "Epoch 53:  75%|▊| 30/40 [00:02<00:00, 14.54it/s, loss=0.136, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 53: 100%|█| 40/40 [00:02<00:00, 17.00it/s, loss=0.136, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 53: 100%|█| 40/40 [00:02<00:00, 17.00it/s, loss=0.136, v_num=, val_loss=0.Epoch    53: Train Loss =   0.136, Val Loss =   0.148\n",
      "Epoch 54:  75%|▊| 30/40 [00:02<00:00, 14.68it/s, loss=0.125, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 54: 100%|█| 40/40 [00:02<00:00, 17.27it/s, loss=0.126, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 54: 100%|█| 40/40 [00:02<00:00, 17.27it/s, loss=0.126, v_num=, val_loss=0.Epoch    54: Train Loss =   0.126, Val Loss =   0.154\n",
      "Epoch 55:  75%|▊| 30/40 [00:02<00:00, 14.75it/s, loss=0.126, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 55: 100%|█| 40/40 [00:02<00:00, 17.27it/s, loss=0.125, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 55: 100%|█| 40/40 [00:02<00:00, 17.26it/s, loss=0.125, v_num=, val_loss=0.Epoch    55: Train Loss =   0.121, Val Loss =   0.136\n",
      "Epoch 56:  75%|▊| 30/40 [00:02<00:00, 14.35it/s, loss=0.121, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 56: 100%|█| 40/40 [00:02<00:00, 16.88it/s, loss=0.12, v_num=, val_loss=0.1\u001b[A\n",
      "Epoch 56: 100%|█| 40/40 [00:02<00:00, 16.87it/s, loss=0.12, v_num=, val_loss=0.1Epoch    56: Train Loss =   0.118, Val Loss =   0.132\n",
      "Epoch 57:  75%|▊| 30/40 [00:02<00:00, 13.96it/s, loss=0.122, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 57: 100%|█| 40/40 [00:02<00:00, 16.41it/s, loss=0.119, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 57: 100%|█| 40/40 [00:02<00:00, 16.40it/s, loss=0.119, v_num=, val_loss=0.Epoch    57: Train Loss =   0.118, Val Loss =   0.175\n",
      "Epoch 58:  75%|▊| 30/40 [00:02<00:00, 14.57it/s, loss=0.114, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 58: 100%|█| 40/40 [00:02<00:00, 17.11it/s, loss=0.117, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 58: 100%|█| 40/40 [00:02<00:00, 17.11it/s, loss=0.117, v_num=, val_loss=0.Epoch    58: Train Loss =   0.111, Val Loss =   0.135\n",
      "Epoch 59:  75%|▊| 30/40 [00:02<00:00, 14.37it/s, loss=0.108, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 40/40 [00:02<00:00, 16.92it/s, loss=0.108, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 59: 100%|█| 40/40 [00:02<00:00, 16.91it/s, loss=0.108, v_num=, val_loss=0.Epoch    59: Train Loss =   0.106, Val Loss =   0.137\n",
      "Epoch 60:  75%|▊| 30/40 [00:02<00:00, 14.30it/s, loss=0.112, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 60: 100%|█| 40/40 [00:02<00:00, 16.55it/s, loss=0.111, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 60: 100%|█| 40/40 [00:02<00:00, 16.54it/s, loss=0.111, v_num=, val_loss=0.Epoch    60: Train Loss =   0.110, Val Loss =   0.121\n",
      "Epoch 61:  75%|▊| 30/40 [00:02<00:00, 14.58it/s, loss=0.103, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 61: 100%|█| 40/40 [00:02<00:00, 17.10it/s, loss=0.0998, v_num=, val_loss=0\u001b[A\n",
      "Epoch 61: 100%|█| 40/40 [00:02<00:00, 17.09it/s, loss=0.0998, v_num=, val_loss=0Epoch    61: Train Loss =   0.104, Val Loss =   0.190\n",
      "Epoch 62:  75%|▊| 30/40 [00:02<00:00, 14.36it/s, loss=0.104, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 62: 100%|█| 40/40 [00:02<00:00, 16.76it/s, loss=0.102, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 62: 100%|█| 40/40 [00:02<00:00, 16.75it/s, loss=0.102, v_num=, val_loss=0.Epoch    62: Train Loss =   0.098, Val Loss =   0.184\n",
      "Epoch 63:  75%|▊| 30/40 [00:00<00:00, 40.94it/s, loss=0.102, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 63: 100%|█| 40/40 [00:01<00:00, 39.30it/s, loss=0.0999, v_num=, val_loss=0\u001b[A\n",
      "Epoch 63: 100%|█| 40/40 [00:01<00:00, 39.26it/s, loss=0.0999, v_num=, val_loss=0Epoch    63: Train Loss =   0.099, Val Loss =   0.215\n",
      "Epoch 64:  75%|▊| 30/40 [00:02<00:00, 13.82it/s, loss=0.0957, v_num=, val_loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 64: 100%|█| 40/40 [00:02<00:00, 16.16it/s, loss=0.0956, v_num=, val_loss=0\u001b[A\n",
      "Epoch 64: 100%|█| 40/40 [00:02<00:00, 16.16it/s, loss=0.0956, v_num=, val_loss=0Epoch    64: Train Loss =   0.094, Val Loss =   0.157\n",
      "Epoch 65:  75%|▊| 30/40 [00:02<00:00, 13.76it/s, loss=0.102, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 65: 100%|█| 40/40 [00:02<00:00, 16.24it/s, loss=0.0995, v_num=, val_loss=0\u001b[A\n",
      "Epoch 65: 100%|█| 40/40 [00:02<00:00, 16.23it/s, loss=0.0995, v_num=, val_loss=0Epoch    65: Train Loss =   0.098, Val Loss =   0.158\n",
      "Epoch 66:  75%|▊| 30/40 [00:02<00:00, 14.48it/s, loss=0.0986, v_num=, val_loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 66: 100%|█| 40/40 [00:02<00:00, 16.98it/s, loss=0.0994, v_num=, val_loss=0\u001b[A\n",
      "Epoch 66: 100%|█| 40/40 [00:02<00:00, 16.97it/s, loss=0.0994, v_num=, val_loss=0Epoch    66: Train Loss =   0.099, Val Loss =   0.117\n",
      "Epoch 67:  75%|▊| 30/40 [00:02<00:00, 14.32it/s, loss=0.0992, v_num=, val_loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 67: 100%|█| 40/40 [00:02<00:00, 16.84it/s, loss=0.101, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 67: 100%|█| 40/40 [00:02<00:00, 16.83it/s, loss=0.101, v_num=, val_loss=0.Epoch    67: Train Loss =   0.098, Val Loss =   0.112\n",
      "Epoch 68:  75%|▊| 30/40 [00:02<00:00, 13.44it/s, loss=0.095, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 68: 100%|█| 40/40 [00:02<00:00, 15.84it/s, loss=0.0913, v_num=, val_loss=0\u001b[A\n",
      "Epoch 68: 100%|█| 40/40 [00:02<00:00, 15.83it/s, loss=0.0913, v_num=, val_loss=0Epoch    68: Train Loss =   0.092, Val Loss =   0.184\n",
      "Epoch 69:  75%|▊| 30/40 [00:02<00:00, 14.42it/s, loss=0.1, v_num=, val_loss=0.18\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 69: 100%|█| 40/40 [00:02<00:00, 16.92it/s, loss=0.101, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 69: 100%|█| 40/40 [00:02<00:00, 16.91it/s, loss=0.101, v_num=, val_loss=0.Epoch    69: Train Loss =   0.097, Val Loss =   0.172\n",
      "Epoch 70:  75%|▊| 30/40 [00:02<00:00, 14.64it/s, loss=0.0983, v_num=, val_loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 70: 100%|█| 40/40 [00:02<00:00, 17.16it/s, loss=0.0973, v_num=, val_loss=0\u001b[A\n",
      "Epoch 70: 100%|█| 40/40 [00:02<00:00, 17.15it/s, loss=0.0973, v_num=, val_loss=0Epoch    70: Train Loss =   0.093, Val Loss =   0.172\n",
      "Epoch 71:  75%|▊| 30/40 [00:02<00:00, 14.52it/s, loss=0.0923, v_num=, val_loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 71: 100%|█| 40/40 [00:02<00:00, 16.99it/s, loss=0.0938, v_num=, val_loss=0\u001b[A\n",
      "Epoch 71: 100%|█| 40/40 [00:02<00:00, 16.99it/s, loss=0.0938, v_num=, val_loss=0Epoch    71: Train Loss =   0.092, Val Loss =   0.151\n",
      "Epoch 72:  75%|▊| 30/40 [00:02<00:00, 14.50it/s, loss=0.0904, v_num=, val_loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 72: 100%|█| 40/40 [00:02<00:00, 16.98it/s, loss=0.0918, v_num=, val_loss=0\u001b[A\n",
      "Epoch 72: 100%|█| 40/40 [00:02<00:00, 16.97it/s, loss=0.0918, v_num=, val_loss=0Epoch    72: Train Loss =   0.090, Val Loss =   0.178\n",
      "Epoch 73:  75%|▊| 30/40 [00:02<00:00, 14.08it/s, loss=0.0871, v_num=, val_loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 73: 100%|█| 40/40 [00:02<00:00, 16.51it/s, loss=0.088, v_num=, val_loss=0.\u001b[A\n",
      "Epoch 73: 100%|█| 40/40 [00:02<00:00, 16.50it/s, loss=0.088, v_num=, val_loss=0.Epoch    73: Train Loss =   0.088, Val Loss =   0.171\n",
      "Epoch 74:  75%|▊| 30/40 [00:00<00:00, 35.66it/s, loss=0.0829, v_num=, val_loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 74: 100%|█| 40/40 [00:01<00:00, 34.02it/s, loss=0.0846, v_num=, val_loss=0\u001b[A\n",
      "Epoch 74: 100%|█| 40/40 [00:01<00:00, 33.99it/s, loss=0.0846, v_num=, val_loss=0Epoch    74: Train Loss =   0.081, Val Loss =   0.153\n",
      "Epoch 75:  75%|▊| 30/40 [00:02<00:00, 14.60it/s, loss=0.094, v_num=, val_loss=0.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 75: 100%|█| 40/40 [00:02<00:00, 17.11it/s, loss=0.0922, v_num=, val_loss=0\u001b[A\n",
      "Epoch 75: 100%|█| 40/40 [00:02<00:00, 17.10it/s, loss=0.0922, v_num=, val_loss=0Epoch    75: Train Loss =   0.089, Val Loss =   0.166\n",
      "Epoch 76:  75%|▊| 30/40 [00:02<00:00, 14.16it/s, loss=0.0866, v_num=, val_loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 76: 100%|█| 40/40 [00:02<00:00, 16.66it/s, loss=0.0869, v_num=, val_loss=0\u001b[A\n",
      "Epoch 76: 100%|█| 40/40 [00:02<00:00, 16.65it/s, loss=0.0869, v_num=, val_loss=0Epoch    76: Train Loss =   0.086, Val Loss =   0.157\n",
      "Monitored metric val_loss did not improve in the last 25 records. Best score: 0.102. Signaling Trainer to stop.\n",
      "Epoch 76: 100%|█| 40/40 [00:02<00:00, 13.85it/s, loss=0.0869, v_num=, val_loss=0\n",
      "Restoring states from the checkpoint path at output/training_logs/d1280_exp_bs32_esp25_wd100/checkpoints/epoch=51-step=1664.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at output/training_logs/d1280_exp_bs32_esp25_wd100/checkpoints/epoch=51-step=1664.ckpt\n",
      "Testing: 0it [00:00, ?it/s]Starting testing...\n",
      "Testing DataLoader 0: 100%|█████████████████████| 12/12 [00:00<00:00, 50.69it/s]Testing complete.\n",
      "Testing DataLoader 0: 100%|█████████████████████| 12/12 [00:00<00:00, 44.13it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11882971972227097   \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m      test_pearson       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6843635439872742    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m      test_spearman      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6502830982208252    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n",
      "Restoring states from the checkpoint path at output/training_logs/d1280_exp_bs32_esp25_wd100/checkpoints/epoch=51-step=1664.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at output/training_logs/d1280_exp_bs32_esp25_wd100/checkpoints/epoch=51-step=1664.ckpt\n",
      "Predicting: 32it [00:00, ?it/s]Starting prediction...\n",
      "Predicting DataLoader 2: 100%|██████████████████| 12/12 [00:00<00:00, 62.28it/s]Prediction complete.\n",
      "Predicting DataLoader 2: 100%|██████████████████| 12/12 [00:00<00:00, 61.77it/s]\n",
      "saving a scatter plot for set: train (1024 variants)\n",
      "saving a scatter plot for set: val (256 variants)\n",
      "saving a scatter plot for set: test (357 variants)\n",
      "            mse  pearsonr        r2  spearmanr\n",
      "set                                           \n",
      "train  0.114704  0.699152  0.457121   0.694843\n",
      "val    0.101959  0.670469  0.413676   0.645642\n",
      "test   0.118830  0.684364  0.422861   0.650283\n"
     ]
    }
   ],
   "source": [
    "# 320 data points\n",
    "!python /home/oscar/projects/strucbio_projects/plm_md_modeling/metl/metl/code/train_target_model.py @finetune_h1_af3_global.txt --enable_progress_bar true --enable_simple_progress_messages --uuid d1280_exp_bs32_esp25_wd100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fc407-6ab1-45e3-8e6a-9b717dca7f00",
   "metadata": {},
   "source": [
    "# Additional recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8e0e5-8bb5-4200-ab45-e559b0f20896",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model selection\n",
    "\n",
    "Selecting the model from the epoch with the lowest validation set loss can help prevent overfitting. It requires having a big enough validation set that provides an accurate estimate of performance. \n",
    "\n",
    "We enabled model selection if the validation set size was ≥ 32 for METL-Local and ≥ 128 for METL-Global. We found the optimization was more stable for METL-Local than METL-Global, thus smaller validation sets were still reliable. \n",
    "\n",
    "Enable model selection by setting argument `--ckpt_monitor val`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f773b-8209-4993-b3f0-994b0ab2b133",
   "metadata": {},
   "source": [
    "## Backbone cutoff for METL-Global\n",
    "Finetuning METL-Global is largely the same as METL-Local, except we recommend using a different threshold for model selection (see above), as well as a different backbone cutoff.\n",
    "\n",
    "For METL-Local, we set `--backbone_cutoff -1`, which attaches the new prediction head immediately after the final fully connected layer. \n",
    "\n",
    "For METL-Global, we recommend setting `--backbone_cutoff -2`, which attaches the new prediction head immediately after the global pooling layer. We found this resulted in better finetuning performance for METL-Global."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a591eb8-3d5e-437f-9189-3c0834f7f447",
   "metadata": {},
   "source": [
    "# Running inference using finetuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958aba4-6881-45d9-a566-9d47f81484dc",
   "metadata": {},
   "source": [
    "We provide two ways to run inference: using our PyTorch Lightning-based inference framework or using your own inference loop. The example below shows you would use your own inference loop. See the [inference.ipynb](inference.ipynb) notebook for additional information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4340777b-fd65-4717-9026-dcdb7f3df581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import inference\n",
    "import encode as enc\n",
    "import utils  # for loading dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b55ce3ad-06f0-4da8-9b23-d5738830f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sg/PycharmProjects/metl/code/inference.py:159: UserWarning: Transforming checkpoint keys: strip_prefix='model.', add_prefix=''\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# the Lightning checkpoint from the finetuning we performed above\n",
    "ckpt_fn = \"output/training_logs/inqx2jYi/checkpoints/epoch=49-step=50.ckpt\"\n",
    "model = inference.load_pytorch_module(ckpt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e2ccfa9-e012-40e6-8f1b-266275418679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3805],\n",
      "        [-0.3623],\n",
      "        [-0.2217]])\n"
     ]
    }
   ],
   "source": [
    "# load the GFP wild-type sequence and the PDB file (needed for 3D RPE)\n",
    "datasets = utils.load_dataset_metadata()\n",
    "wt = datasets[\"avgfp\"][\"wt_aa\"]\n",
    "wt_offset = datasets[\"avgfp\"][\"wt_ofs\"]\n",
    "pdb_fn = datasets[\"avgfp\"][\"pdb_fn\"]\n",
    "\n",
    "# some example GFP variants to compute the scores for\n",
    "variants = [\"E3K,G102S\",\n",
    "            \"T36P,S203T,K207R\",\n",
    "            \"V10A,D19G,F25S,E113V\"]\n",
    "\n",
    "# encode the variants into the integer sequences format\n",
    "# to feed into the model\n",
    "encoded_variants = enc.encode(\n",
    "    encoding=\"int_seqs\",\n",
    "    variants=variants,\n",
    "    wt_aa=wt,\n",
    "    wt_offset=wt_offset,\n",
    "    indexing=\"0_indexed\"\n",
    ")\n",
    "\n",
    "# set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# no need to compute gradients for inference\n",
    "with torch.no_grad():\n",
    "    # note we are specifying the pdb_fn because this model uses 3D RPE\n",
    "    predictions = model(torch.tensor(encoded_variants), pdb_fn=pdb_fn)\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
