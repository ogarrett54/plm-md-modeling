{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27bbc2b0",
   "metadata": {},
   "source": [
    "# Fine-tuning ESMDance Models on Yeast Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321c6dc",
   "metadata": {},
   "source": [
    "Objectives\n",
    "- Fine-tune ESMDance base model and mutant NMA expert model on yeast data.\n",
    "- Compare to ESM2 base models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f489412",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e0b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aeef79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_sequence</th>\n",
       "      <th>enrichment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>1.468796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GLKRIIVIKVAREGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.415944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GLKRIIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.389615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>1.359651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.343857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3955</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>-1.041749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3956</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEA...</td>\n",
       "      <td>-1.041749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3957</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>-1.057543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3958</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKTEV...</td>\n",
       "      <td>-1.057543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3959</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKTEV...</td>\n",
       "      <td>-1.057543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3960 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            aa_sequence  enrichment_score\n",
       "0     GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...          1.468796\n",
       "1     GLKRIIVIKVAREGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.415944\n",
       "2     GLKRIIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.389615\n",
       "3     GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...          1.359651\n",
       "4     GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.343857\n",
       "...                                                 ...               ...\n",
       "3955  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...         -1.041749\n",
       "3956  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEA...         -1.041749\n",
       "3957  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...         -1.057543\n",
       "3958  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKTEV...         -1.057543\n",
       "3959  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKTEV...         -1.057543\n",
       "\n",
       "[3960 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir = Path('yeast_data')\n",
    "df = pd.read_csv(str(input_dir / 'avrpikC_full.csv'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58e0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BindingDataset(Dataset):\n",
    "    \"\"\"Dataset for the final binding prediction task.\"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        # Ensure the columns exist\n",
    "        assert 'aa_sequence' in self.df.columns, \"DataFrame must have 'aa_sequence' column.\"\n",
    "        assert 'enrichment_score' in self.df.columns, \"DataFrame must have 'enrichment_score' column.\"\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.df.iloc[idx]['aa_sequence']\n",
    "        # The label is a single float value\n",
    "        label = torch.tensor(self.df.iloc[idx]['enrichment_score'], dtype=torch.float)\n",
    "\n",
    "        # Tokenize the sequence\n",
    "        tokenized_output = self.tokenizer(\n",
    "            sequence,\n",
    "            max_length=160, # Use a fixed max length. 158 residues plus 2 extra tokens\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Return input_ids: attention masks, removing batch dimension\n",
    "        inputs = {key: val.squeeze(0) for key, val in tokenized_output.items()}\n",
    "        \n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845b69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = BindingDataset(df)\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a9560",
   "metadata": {},
   "source": [
    "## Approach 1: Frozen ESM and ESMDance\n",
    "Use base ESM, ESMDance, and fine-tuned NMA as a feature extractor for simple linear regression head. This method pools each of the features into a single numerical representation, which doesn't actually make that much sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ca63438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from scripts.esmdance_flex_model import ESMwrap # Customized ESMDance model definition for dynamic config usage\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, original_model_config, nma_model_config, nma_model_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Instantiate the original ESMDance with its 50/13 res/pair features config\n",
    "        print(\"Initializing original ESMDance model...\")\n",
    "        self.original_esmdance = ESMwrap(model_config=original_model_config)\n",
    "        self.original_esmdance = self.original_esmdance.from_pretrained(\"ChaoHou/ESMDance\", model_config=original_model_config)\n",
    "\n",
    "        # Instantiate the NMA-tuned model with the 3/3 res/pair features config\n",
    "        print(f\"Initializing custom NMA-tuned model from {nma_model_path}...\")\n",
    "        self.nma_esmdance = ESMwrap(model_config=nma_model_config)\n",
    "        self.nma_esmdance.load_state_dict(torch.load(nma_model_path, map_location='cpu'))\n",
    "\n",
    "        # Freeze parameters\n",
    "        print(\"Freezing all parameters in the feature extractor...\")\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.eval()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            md_preds = self.original_esmdance(inputs) # Get features from original ESMDance\n",
    "            nma_preds = self.nma_esmdance(inputs) # Get features from the mutant, fine-tuned ESMDance\n",
    "            raw_embeddings = self.original_esmdance.esm2(**inputs).last_hidden_state # Get features from ESM2\n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1) # Add dimension to allow matrix multiplication with raw embeddings\n",
    "            \n",
    "            # Pool raw embeddings, averaging across residues to give each sequence a single representation\n",
    "            pooled_embed = (raw_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "\n",
    "            # Gather and unify dimensions for all MD residue features\n",
    "            original_res_keys = self.original_esmdance.config['training']['res_feature_idx'].keys()\n",
    "            md_tensors_to_cat = []\n",
    "            for k in original_res_keys:\n",
    "                tensor = md_preds[k]\n",
    "                if tensor.dim() == 2:\n",
    "                    # If tensor is 2D (e.g., shape [B, L]), add a feature dimension\n",
    "                    md_tensors_to_cat.append(tensor.unsqueeze(-1))\n",
    "                else:\n",
    "                    # If tensor is already 3D (e.g., shape [B, L, F]), add it as is\n",
    "                    md_tensors_to_cat.append(tensor)\n",
    "            md_res_features = torch.cat(md_tensors_to_cat, dim=-1)\n",
    "\n",
    "            # Gather and unify dimensions for all 3 NMA residue features\n",
    "            nma_res_keys = self.nma_esmdance.config['training']['res_feature_idx'].keys()\n",
    "            nma_tensors_to_cat = []\n",
    "            for k in nma_res_keys:\n",
    "                tensor = nma_preds[k]\n",
    "                if tensor.dim() == 2:\n",
    "                    nma_tensors_to_cat.append(tensor.unsqueeze(-1))\n",
    "                else:\n",
    "                    nma_tensors_to_cat.append(tensor)\n",
    "            nma_res_features = torch.cat(nma_tensors_to_cat, dim=-1)\n",
    "\n",
    "            # Pool the correctly shaped features\n",
    "            pooled_md_res = (md_res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            pooled_nma_res = (nma_res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            \n",
    "            # Concatenate all features into one vector\n",
    "            final_feature_vector = torch.cat([pooled_embed, pooled_md_res, pooled_nma_res], dim=-1)\n",
    "            \n",
    "        return final_feature_vector\n",
    "\n",
    "\n",
    "class BindingHead(nn.Module):\n",
    "    \"\"\"\n",
    "    The small regression head that we will train.\n",
    "    It takes the concatenated feature vector as input.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features):\n",
    "        super().__init__()\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(input_features, input_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(input_features // 2, 1) # Output a single value for enrichment\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.regression_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "626816ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing original ESMDance model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing custom NMA-tuned model from models/esmdance-mutant-nma-fine-tuned/esmdance_fine-tuned_with_nma_data.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing all parameters in the feature extractor...\n",
      "Determining feature vector size...\n",
      "Concatenated feature vector size: 533\n",
      "Starting training of the binding head for 5 epochs...\n",
      "\n",
      "Epoch 1/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|██████████| 223/223 [00:19<00:00, 11.27it/s]\n",
      "[Val]: 100%|██████████| 25/25 [00:02<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1986 | Validation Loss: 0.1919 | Spearman Correlation: 0.5796\n",
      "\n",
      "Epoch 2/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|██████████| 223/223 [00:19<00:00, 11.26it/s]\n",
      "[Val]: 100%|██████████| 25/25 [00:02<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1803 | Validation Loss: 0.1718 | Spearman Correlation: 0.5766\n",
      "\n",
      "Epoch 3/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|██████████| 223/223 [00:19<00:00, 11.28it/s]\n",
      "[Val]: 100%|██████████| 25/25 [00:02<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1651 | Validation Loss: 0.1576 | Spearman Correlation: 0.5845\n",
      "\n",
      "Epoch 4/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|██████████| 223/223 [00:22<00:00,  9.79it/s]\n",
      "[Val]: 100%|██████████| 25/25 [00:02<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1546 | Validation Loss: 0.1478 | Spearman Correlation: 0.5947\n",
      "\n",
      "Epoch 5/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|██████████| 223/223 [00:19<00:00, 11.29it/s]\n",
      "[Val]: 100%|██████████| 25/25 [00:02<00:00,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1488 | Validation Loss: 0.1413 | Spearman Correlation: 0.6044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from scripts.base_config import config as base_config\n",
    "from scripts.nma_finetuned_config import config as nma_config\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "# --- SETUP ---\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "nma_model_path = 'models/esmdance-mutant-nma-fine-tuned/esmdance_fine-tuned_with_nma_data.pth' # Path to your NMA-tuned model\n",
    "\n",
    "# Initialize the frozen feature extractor and the trainable binding head\n",
    "feature_extractor = FeatureExtractor(original_model_config=base_config, \n",
    "                                     nma_model_config=nma_config, \n",
    "                                     nma_model_path=nma_model_path).to(device)\n",
    "\n",
    "# Determine the input size for the binding head after one forward pass\n",
    "print(\"Determining feature vector size...\")\n",
    "with torch.no_grad():\n",
    "    dummy_inputs, _ = next(iter(train_loader))\n",
    "    dummy_inputs = {k: v.to(device) for k,v in dummy_inputs.items()}\n",
    "    dummy_feature_vector = feature_extractor(dummy_inputs,)\n",
    "    feature_vector_size = dummy_feature_vector.shape[1]\n",
    "\n",
    "print(f\"Concatenated feature vector size: {feature_vector_size}\")\n",
    "binding_head = BindingHead(feature_vector_size).to(device)\n",
    "\n",
    "# --- LOSS AND OPTIMIZER ---\n",
    "# MSE is a good loss function for regression tasks like enrichment scores\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Pass ONLY the parameters of the binding_head to the optimizer\n",
    "optimizer = AdamW(binding_head.parameters(), lr=1e-4)\n",
    "\n",
    "# --- TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting training of the binding head for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{num_epochs}\\n----------------------------')\n",
    "    # =======================================\n",
    "    #               TRAINING\n",
    "    # =======================================\n",
    "    binding_head.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"[Train]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device).unsqueeze(1) # Reshape labels for MSELoss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        feature_vector = feature_extractor(inputs)\n",
    "        predictions = binding_head(feature_vector)\n",
    "        \n",
    "        loss = loss_function(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # =======================================\n",
    "    #              VALIDATION\n",
    "    # =======================================\n",
    "    binding_head.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"[Val]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            feature_vector = feature_extractor(inputs)\n",
    "            predictions = binding_head(feature_vector)\n",
    "            \n",
    "            total_val_loss += loss_function(predictions, labels).item()\n",
    "            epoch_predictions.append(predictions.cpu().detach())\n",
    "            epoch_labels.append(labels.cpu().detach())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    # Concatenate all batch tensors into single, large tensors\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    \n",
    "    # Calculate Spearman's rank correlation coefficient\n",
    "    # spearmanr returns two values: the correlation and the p-value\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    # --- Print metrics ---\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f91c695",
   "metadata": {},
   "source": [
    "## Approach 2: Fine-tuned ESMDance with Attention\n",
    "This model loads the fine-tuned ESMDance model on the mutant NMA data with attention to integrate the pair-wise and residue-level outputs of ESMDance. It also unfreezes the final two layers of ESM2 for improved encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a817a1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing custom NMA-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing ESM-2 base and unfreezing top 2 layers and prediction heads...\n",
      "Total trainable parameters: 9,623,763\n",
      "Starting fine-tuning for 5 epochs...\n",
      "\n",
      "Epoch 1/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]:   0%|          | 0/223 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/oscar/miniconda3/envs/ml/lib/python3.11/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "[Training]: 100%|██████████| 223/223 [00:51<00:00,  4.34it/s]\n",
      "[Validation]: 100%|██████████| 25/25 [00:03<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1883 | Validation Loss: 0.1402 | Spearman Correlation: 0.6600\n",
      "\n",
      "Epoch 2/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|██████████| 223/223 [00:49<00:00,  4.52it/s]\n",
      "[Validation]: 100%|██████████| 25/25 [00:03<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1297 | Validation Loss: 0.1057 | Spearman Correlation: 0.7073\n",
      "\n",
      "Epoch 3/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|██████████| 223/223 [00:48<00:00,  4.61it/s]\n",
      "[Validation]: 100%|██████████| 25/25 [00:03<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1132 | Validation Loss: 0.1033 | Spearman Correlation: 0.7410\n",
      "\n",
      "Epoch 4/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|██████████| 223/223 [00:23<00:00,  9.69it/s]\n",
      "[Validation]: 100%|██████████| 25/25 [00:01<00:00, 17.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1009 | Validation Loss: 0.0864 | Spearman Correlation: 0.7583\n",
      "\n",
      "Epoch 5/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|██████████| 223/223 [00:20<00:00, 10.82it/s]\n",
      "[Validation]: 100%|██████████| 25/25 [00:01<00:00, 17.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0971 | Validation Loss: 0.0861 | Spearman Correlation: 0.7762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Import your custom NMA-tuned config and the base model class ---\n",
    "from scripts.nma_finetuned_config import config as nma_config\n",
    "from scripts.esmdance_flex_model import ESMwrap\n",
    "\n",
    "class AttentionBindingHead(nn.Module):\n",
    "    \"\"\"The advanced binding head that processes per-residue and per-pair features.\"\"\"\n",
    "    def __init__(self, embed_dim, pair_dim, num_heads=7):\n",
    "        super().__init__()\n",
    "        self.pair_bias_net = nn.Sequential(\n",
    "            nn.Linear(pair_dim, num_heads), nn.ReLU(), nn.Linear(num_heads, num_heads)\n",
    "        )\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4), nn.ReLU(), nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2), nn.ReLU(), nn.Dropout(0.2), nn.Linear(embed_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, residue_features, pair_features, attention_mask):\n",
    "        pair_bias = self.pair_bias_net(pair_features).permute(0, 3, 1, 2)\n",
    "        batch_size, num_heads, seq_len, _ = pair_bias.shape\n",
    "        pair_bias = pair_bias.reshape(batch_size * num_heads, seq_len, seq_len)\n",
    "        padding_mask = (attention_mask == 0)\n",
    "        \n",
    "        attn_output, _ = self.attention(\n",
    "            residue_features, residue_features, residue_features,\n",
    "            key_padding_mask=padding_mask, attn_mask=pair_bias\n",
    "        )\n",
    "        residue_features = self.layer_norm1(residue_features + attn_output)\n",
    "        ffn_output = self.ffn(residue_features)\n",
    "        residue_features = self.layer_norm2(residue_features + ffn_output)\n",
    "        cls_token_embedding = residue_features[:, 0, :]\n",
    "        return self.regressor(cls_token_embedding)\n",
    "\n",
    "class NMAFineTuningForBinding(nn.Module):\n",
    "    \"\"\"\n",
    "    A single, end-to-end model for fine-tuning ESMDance on binding data.\n",
    "    \"\"\"\n",
    "    def __init__(self, nma_model_config: dict, nma_model_path: str):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Instantiate your NMA-tuned model with its 3/3 config\n",
    "        print(\"Initializing custom NMA-tuned model...\")\n",
    "        self.esmdance_base = ESMwrap(model_config=nma_model_config)\n",
    "        self.esmdance_base.load_state_dict(torch.load(nma_model_path, map_location='cpu'))\n",
    "\n",
    "        # 2. --- SELECTIVE UNFREEZING ---\n",
    "        print(\"Freezing ESM-2 base and unfreezing top 2 layers and prediction heads...\")\n",
    "        # First, freeze the entire ESM-2 sub-module\n",
    "        for param in self.esmdance_base.esm2.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Then, unfreeze only the parameters of the last two transformer layers\n",
    "        num_layers_to_unfreeze = 2\n",
    "        for layer in self.esmdance_base.esm2.encoder.layer[-num_layers_to_unfreeze:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # NOTE: The ESMDance prediction heads (`res_pred_nn`, etc.) are separate from\n",
    "        # `esm2` and will be trainable by default, which is what we want.\n",
    "\n",
    "        # 3. --- INITIALIZE THE BINDING HEAD ---\n",
    "        embed_dim = nma_model_config['model_35M']['embed_dim']\n",
    "        res_out_dim = nma_model_config['model_35M']['res_out_dim']   # This is 3\n",
    "        pair_out_dim = nma_model_config['model_35M']['pair_out_dim'] # This is 3\n",
    "        \n",
    "        self.binding_head = AttentionBindingHead(\n",
    "            embed_dim=embed_dim + res_out_dim, # 480 + 3 = 483\n",
    "            pair_dim=pair_out_dim              # 3\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        base_outputs = self.esmdance_base(inputs)\n",
    "        raw_embeddings = self.esmdance_base.esm2(**inputs).last_hidden_state\n",
    "        \n",
    "        # Gather NMA residue features (3 of them)\n",
    "        res_keys = self.esmdance_base.config['training']['res_feature_idx'].keys()\n",
    "        res_tensors = [base_outputs[k].unsqueeze(-1) if base_outputs[k].dim() == 2 else base_outputs[k] for k in res_keys]\n",
    "        predicted_res_features = torch.cat(res_tensors, dim=-1)\n",
    "        \n",
    "        # Gather NMA pair features\n",
    "        pair_keys = self.esmdance_base.config['training']['pair_feature_idx'].keys()\n",
    "        \n",
    "        # Use torch.cat to join along the existing feature dimension (the last one)\n",
    "        predicted_pair_features = torch.cat([base_outputs[k] for k in pair_keys], dim=-1)\n",
    "        \n",
    "        # Combine inputs for the head\n",
    "        final_residue_features = torch.cat([raw_embeddings, predicted_res_features], dim=-1)\n",
    "        \n",
    "        return self.binding_head(\n",
    "            residue_features=final_residue_features,\n",
    "            pair_features=predicted_pair_features,\n",
    "            attention_mask=inputs['attention_mask']\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "#                            MAIN TRAINING SCRIPT\n",
    "# =============================================================================\n",
    "# --- SETUP, DATA LOADING ---\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "nma_model_path = 'models/esmdance-mutant-nma-fine-tuned_relaxed/esmdance_fine-tuned_with_nma_data.pth'\n",
    "\n",
    "# The new model initialization is now much cleaner\n",
    "model = NMAFineTuningForBinding(\n",
    "    nma_model_config=nma_config,\n",
    "    nma_model_path=nma_model_path\n",
    ").to(device)\n",
    "\n",
    "# --- LOSS AND OPTIMIZER ---\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5) # Use a smaller LR for fine-tuning\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# --- TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{num_epochs}\\n----------------------------')\n",
    "    # =======================================\n",
    "    #               TRAINING\n",
    "    # =======================================\n",
    "    model.train() # Set the binding head to training mode (activates dropout)\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"[Training]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device).unsqueeze(1) # Reshape labels for MSELoss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        loss = loss_function(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # =======================================\n",
    "    #              VALIDATION\n",
    "    # =======================================\n",
    "    model.eval() # Set the binding head to evaluation mode (disables dropout)\n",
    "    total_val_loss = 0\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"[Validation]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            total_val_loss += loss_function(predictions, labels).item()\n",
    "\n",
    "            # Collect predictions and labels for Spearman correlation\n",
    "            epoch_predictions.append(predictions.cpu().detach())\n",
    "            epoch_labels.append(labels.cpu().detach())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    # Calculate Spearman Correlation\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    print(f\"Training Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762161ff",
   "metadata": {},
   "source": [
    "Without relax (5 epochs): Validation Loss: 0.0964 | Spearman Correlation: 0.7496"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d01f60",
   "metadata": {},
   "source": [
    "With relax (5 epochs): Validation loss: 0.0861 | Spearman correlation: 0.7762"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a18f3a",
   "metadata": {},
   "source": [
    "Compared to the most similar baseline, trainable ESMDance without pooling (Validation Loss: 0.0957 | Spearman Correlation: 0.7465), the model fine-tuned on relaxed structures does a little bit better, at least on this run (pending replicates with different seeds). It also does slightly better than training the full 8M ESM2 (Validation Loss: 0.0859 | Spearman Correlation: 0.7586) and 35M ESM2 with two layers unfrozen (Validation Loss: 0.1062 | Spearman Correlation: 0.7602)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaed543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
