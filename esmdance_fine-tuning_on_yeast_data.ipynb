{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27bbc2b0",
   "metadata": {},
   "source": [
    "# Fine-tuning ESMDance Models on Yeast Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321c6dc",
   "metadata": {},
   "source": [
    "Objectives\n",
    "- Fine-tune ESMDance base model and mutant NMA expert model on yeast data.\n",
    "- Evaluate data efficiency of training.\n",
    "- Evaluate immunogenicity accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e0b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3aeef79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_sequence</th>\n",
       "      <th>enrichment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>1.468796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GLKRIIVIKVAREGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.415944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GLKRIIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.389615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>1.359651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.343857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3955</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>-1.041749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3956</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEA...</td>\n",
       "      <td>-1.041749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3957</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>-1.057543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3958</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKTEV...</td>\n",
       "      <td>-1.057543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3959</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKTEV...</td>\n",
       "      <td>-1.057543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3960 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            aa_sequence  enrichment_score\n",
       "0     GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...          1.468796\n",
       "1     GLKRIIVIKVAREGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.415944\n",
       "2     GLKRIIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.389615\n",
       "3     GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...          1.359651\n",
       "4     GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.343857\n",
       "...                                                 ...               ...\n",
       "3955  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...         -1.041749\n",
       "3956  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEA...         -1.041749\n",
       "3957  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...         -1.057543\n",
       "3958  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKTEV...         -1.057543\n",
       "3959  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKTEV...         -1.057543\n",
       "\n",
       "[3960 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir = Path('yeast_data')\n",
    "df = pd.read_csv(str(input_dir / 'avrpikC_full.csv'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "564203e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58e0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BindingDataset(Dataset):\n",
    "    \"\"\"Dataset for the final binding prediction task.\"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        # Ensure the columns exist\n",
    "        assert 'aa_sequence' in self.df.columns, \"DataFrame must have 'aa_sequence' column.\"\n",
    "        assert 'enrichment_score' in self.df.columns, \"DataFrame must have 'enrichment_score' column.\"\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.df.iloc[idx]['aa_sequence']\n",
    "        # The label is a single float value\n",
    "        label = torch.tensor(self.df.iloc[idx]['enrichment_score'], dtype=torch.float)\n",
    "\n",
    "        # Tokenize the sequence\n",
    "        tokenized_output = self.tokenizer(\n",
    "            sequence,\n",
    "            truncation=True,\n",
    "            max_length=1024, # Use a fixed max length\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        inputs = {key: val.squeeze(0) for key, val in tokenized_output.items()}\n",
    "        \n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ca63438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from scripts.esmdance_flex_model import ESMwrap\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, original_model_config, nma_model_config, nma_model_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Instantiate the original ESMDance with its 50/13 config\n",
    "        print(\"Initializing original ESMDance model...\")\n",
    "        self.original_esmdance = ESMwrap(model_config=original_model_config)\n",
    "        self.original_esmdance = self.original_esmdance.from_pretrained(\"ChaoHou/ESMDance\",model_config=original_model_config)\n",
    "\n",
    "        # 2. Instantiate your NMA-tuned model using THE SAME CLASS but with the 3/3 config\n",
    "        print(f\"Initializing custom NMA-tuned model from {nma_model_path}...\")\n",
    "        self.nma_esmdance = ESMwrap(model_config=nma_model_config)\n",
    "        \n",
    "        # Load your fine-tuned weights\n",
    "        self.nma_esmdance.load_state_dict(torch.load(nma_model_path, map_location='cpu'))\n",
    "\n",
    "        # 3. Freeze ALL parameters\n",
    "        print(\"Freezing all parameters in the feature extractor...\")\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.eval()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Extract features from both models\n",
    "        with torch.no_grad():\n",
    "            md_preds = self.original_esmdance(inputs)\n",
    "            nma_preds = self.nma_esmdance(inputs)\n",
    "            raw_embeddings = self.original_esmdance.esm2(**inputs).last_hidden_state\n",
    "            \n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "            \n",
    "            # Pool raw embeddings\n",
    "            pooled_embed = (raw_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "\n",
    "            # --- THIS SECTION IS NOW CORRECTED ---\n",
    "            \n",
    "            # 1. Correctly gather and unify dimensions for all MD residue features\n",
    "            original_res_keys = self.original_esmdance.config['training']['res_feature_idx'].keys()\n",
    "            md_tensors_to_cat = []\n",
    "            for k in original_res_keys:\n",
    "                tensor = md_preds[k]\n",
    "                if tensor.dim() == 2:\n",
    "                    # If tensor is 2D (e.g., shape [B, L]), add a feature dimension\n",
    "                    md_tensors_to_cat.append(tensor.unsqueeze(-1))\n",
    "                else:\n",
    "                    # If tensor is already 3D (e.g., shape [B, L, F]), add it as is\n",
    "                    md_tensors_to_cat.append(tensor)\n",
    "            md_res_features = torch.cat(md_tensors_to_cat, dim=-1)\n",
    "\n",
    "            # 2. Correctly gather and unify dimensions for all 3 NMA residue features\n",
    "            nma_res_keys = self.nma_esmdance.config['training']['res_feature_idx'].keys()\n",
    "            nma_tensors_to_cat = []\n",
    "            for k in nma_res_keys:\n",
    "                tensor = nma_preds[k]\n",
    "                if tensor.dim() == 2:\n",
    "                    nma_tensors_to_cat.append(tensor.unsqueeze(-1))\n",
    "                else:\n",
    "                    nma_tensors_to_cat.append(tensor)\n",
    "            nma_res_features = torch.cat(nma_tensors_to_cat, dim=-1)\n",
    "            \n",
    "            # --- END OF CORRECTION ---\n",
    "\n",
    "            # Now, pool the correctly shaped features\n",
    "            pooled_md_res = (md_res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            pooled_nma_res = (nma_res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            \n",
    "            # Concatenate all features into one vector\n",
    "            final_feature_vector = torch.cat([pooled_embed, pooled_md_res, pooled_nma_res], dim=-1)\n",
    "            \n",
    "        return final_feature_vector\n",
    "\n",
    "\n",
    "class BindingHead(nn.Module):\n",
    "    \"\"\"\n",
    "    The small regression head that we will train.\n",
    "    It takes the concatenated feature vector as input.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features):\n",
    "        super().__init__()\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(input_features, input_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(input_features // 2, 1) # Output a single value for enrichment\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.regression_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "626816ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing original ESMDance model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing custom NMA-tuned model from models/esmdance-mutant-nma-fine-tuned/esmdance_fine-tuned_with_nma_data.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing all parameters in the feature extractor...\n",
      "Determining feature vector size...\n",
      "Concatenated feature vector size: 533\n",
      "Starting training of the binding head for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 223/223 [00:18<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Val]: 100%|██████████| 25/25 [00:02<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss: 0.1991 | Spearman Correlation: 0.5811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 223/223 [00:18<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Val]: 100%|██████████| 25/25 [00:02<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss: 0.1795 | Spearman Correlation: 0.5753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 223/223 [00:21<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Val]: 100%|██████████| 25/25 [00:02<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss: 0.1645 | Spearman Correlation: 0.5827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 223/223 [00:18<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Val]: 100%|██████████| 25/25 [00:02<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Loss: 0.1533 | Spearman Correlation: 0.5897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 223/223 [00:18<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Val]: 100%|██████████| 25/25 [00:02<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Loss: 0.1478 | Spearman Correlation: 0.5977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from scripts.base_config import config as base_config\n",
    "from scripts.nma_finetuned_config import config as nma_config\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. DATA LOADING ---\n",
    "# Load your dataframe with 'aa_sequence' and 'enrichment_score' columns\n",
    "full_dataset = BindingDataset(df)\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# --- 3. MODEL INITIALIZATION ---\n",
    "nma_model_path = 'models/esmdance-mutant-nma-fine-tuned/esmdance_fine-tuned_with_nma_data.pth' # Path to your NMA-tuned model\n",
    "\n",
    "# Initialize the frozen feature extractor and the trainable binding head\n",
    "feature_extractor = FeatureExtractor(original_model_config=base_config, \n",
    "                                     nma_model_config=nma_config, \n",
    "                                     nma_model_path=nma_model_path).to(device)\n",
    "\n",
    "# We need to determine the input size for the binding head after one forward pass\n",
    "print(\"Determining feature vector size...\")\n",
    "with torch.no_grad():\n",
    "    dummy_inputs, _ = next(iter(train_loader))\n",
    "    dummy_inputs = {k: v.to(device) for k,v in dummy_inputs.items()}\n",
    "    dummy_feature_vector = feature_extractor(dummy_inputs,)\n",
    "    feature_vector_size = dummy_feature_vector.shape[1]\n",
    "\n",
    "print(f\"Concatenated feature vector size: {feature_vector_size}\")\n",
    "binding_head = BindingHead(feature_vector_size).to(device)\n",
    "\n",
    "# --- 4. LOSS AND OPTIMIZER ---\n",
    "# MSE is a good loss function for regression tasks like enrichment scores\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# CRITICAL: Pass ONLY the parameters of the binding_head to the optimizer\n",
    "optimizer = AdamW(binding_head.parameters(), lr=1e-4)\n",
    "\n",
    "# --- 5. TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting training of the binding head for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    binding_head.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1} [Train]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device).unsqueeze(1) # Reshape labels for MSELoss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        feature_vector = feature_extractor(inputs)\n",
    "        predictions = binding_head(feature_vector)\n",
    "        \n",
    "        loss = loss_function(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    binding_head.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch + 1} [Val]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            feature_vector = feature_extractor(inputs)\n",
    "            predictions = binding_head(feature_vector)\n",
    "            \n",
    "            total_val_loss += loss_function(predictions, labels).item()\n",
    "            epoch_predictions.append(predictions.cpu().detach())\n",
    "            epoch_labels.append(labels.cpu().detach())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    # Concatenate all batch tensors into single, large tensors\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    \n",
    "    # Calculate Spearman's rank correlation coefficient\n",
    "    # spearmanr returns two values: the correlation and the p-value\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    # --- UPDATED: Print both metrics ---\n",
    "    print(f\"Epoch {epoch + 1} Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12378a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing ESM-2 35M for Sequence Classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the ESM-2 base model layers...\n",
      "Total trainable parameters: 231,361\n",
      "Starting fine-tuning for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 223/223 [00:02<00:00, 91.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Val]: 100%|██████████| 25/25 [00:00<00:00, 44.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2098 | Spearman Correlation: 0.5577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 223/223 [00:05<00:00, 43.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Val]: 100%|██████████| 25/25 [00:00<00:00, 42.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2086 | Spearman Correlation: 0.5696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 223/223 [00:05<00:00, 43.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Val]: 100%|██████████| 25/25 [00:00<00:00, 44.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1928 | Spearman Correlation: 0.5754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 223/223 [00:05<00:00, 43.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Val]: 100%|██████████| 25/25 [00:00<00:00, 44.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1897 | Spearman Correlation: 0.5794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 223/223 [00:05<00:00, 43.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Val]: 100%|██████████| 25/25 [00:00<00:00, 43.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1760 | Spearman Correlation: 0.5856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmForSequenceClassification\n",
    "\n",
    "# --- SETUP ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- DATA LOADING ---\n",
    "# Load your dataframe with 'aa_sequence' and 'enrichment_score' columns\n",
    "#full_dataset = BindingDataset(df)\n",
    "#\n",
    "#train_size = int(0.9 * len(full_dataset))\n",
    "#val_size = len(full_dataset) - train_size\n",
    "#train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "#\n",
    "## NOTE: The collate function is handled automatically by the DataLoader for this dataset\n",
    "#train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "print(\"Initializing ESM-2 35M for Sequence Classification...\")\n",
    "\n",
    "model = EsmForSequenceClassification.from_pretrained(\n",
    "    \"facebook/esm2_t12_35M_UR50D\",\n",
    "    num_labels=1,                # We are predicting one continuous value.\n",
    "    problem_type=\"regression\"    # Configure the model for regression.\n",
    ").to(device)\n",
    "\n",
    "# --- FREEZE THE BASE MODEL (for fair comparison) ---\n",
    "print(\"Freezing the ESM-2 base model layers...\")\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"esm.\"): # This freezes all parameters of the main ESM body\n",
    "        param.requires_grad = False\n",
    "\n",
    "# --- LOSS AND OPTIMIZER ---\n",
    "# We will let the model calculate its own loss during training, but define it for validation\n",
    "loss_function = nn.MSELoss() \n",
    "\n",
    "# The optimizer will automatically ignore frozen parameters\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Count trainable parameters to confirm the base is frozen\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# --- TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1} [Train]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # The model automatically calculates loss when labels are provided\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch + 1} [Val]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get model predictions (logits)\n",
    "            outputs = model(**inputs)\n",
    "            predictions = outputs.logits\n",
    "            \n",
    "            # Calculate validation loss manually\n",
    "            total_val_loss += loss_function(predictions.squeeze(), labels).item()\n",
    "            \n",
    "            # Collect predictions and labels for Spearman correlation\n",
    "            epoch_predictions.append(predictions.cpu())\n",
    "            epoch_labels.append(labels.cpu())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    # Calculate Spearman Correlation\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34d22e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Performing dry run to determine feature vector size...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determined concatenated feature vector size: 530\n",
      "Initializing base ESMDance model with original 50/13 heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original weights from pretrained_weights/esmdance_update_60000.pt...\n",
      "Initializing trainable binding head with input size 530...\n",
      "Total trainable parameters: 140,981\n",
      "Starting fine-tuning for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Training]: 100%|██████████| 223/223 [00:10<00:00, 21.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 0.1976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Validation]: 100%|██████████| 25/25 [00:01<00:00, 16.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss: 0.1994 | Spearman Correlation: 0.5226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Training]: 100%|██████████| 223/223 [00:13<00:00, 16.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Loss: 0.1784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Validation]: 100%|██████████| 25/25 [00:01<00:00, 16.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss: 0.1771 | Spearman Correlation: 0.5228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Training]: 100%|██████████| 223/223 [00:10<00:00, 20.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training Loss: 0.1624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Validation]: 100%|██████████| 25/25 [00:01<00:00, 16.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss: 0.1638 | Spearman Correlation: 0.5301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Training]: 100%|██████████| 223/223 [00:13<00:00, 16.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training Loss: 0.1517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Validation]: 100%|██████████| 25/25 [00:01<00:00, 16.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Loss: 0.1551 | Spearman Correlation: 0.5399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Training]: 100%|██████████| 223/223 [00:10<00:00, 20.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training Loss: 0.1462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Validation]: 100%|██████████| 25/25 [00:01<00:00, 16.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Loss: 0.1500 | Spearman Correlation: 0.5493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "import importlib.util\n",
    "\n",
    "# --- Import the base model class ---\n",
    "from scripts.esmdance_base_model import ESMwrap\n",
    "from scripts.base_config import config as base_config\n",
    "\n",
    "# =============================================================================\n",
    "# 2. NEW MODEL FOR FINE-TUNING\n",
    "# =============================================================================\n",
    "class ESMDanceForBinding(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper model that uses the pre-trained ESMDance as a frozen feature extractor\n",
    "    and adds a new, trainable regression head on top.\n",
    "    \n",
    "    This version initializes all layers in the constructor for robustness.\n",
    "    \"\"\"\n",
    "    # CRITICAL CHANGE: We now pass the feature vector size during initialization\n",
    "    def __init__(self, config, feature_vector_size: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        print(\"Initializing base ESMDance model with original 50/13 heads...\")\n",
    "        self.esmdance_base = ESMwrap(esm2_select='model_35M', model_select='esmdance')\n",
    "        \n",
    "        original_weights_path = 'pretrained_weights/esmdance_update_60000.pt'\n",
    "        print(f\"Loading original weights from {original_weights_path}...\")\n",
    "        self.esmdance_base.load_state_dict(torch.load(original_weights_path, map_location='cpu'))\n",
    "\n",
    "        # Freeze all parameters of the base model\n",
    "        for param in self.esmdance_base.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # CRITICAL CHANGE: Initialize the binding head immediately in __init__\n",
    "        print(f\"Initializing trainable binding head with input size {feature_vector_size}...\")\n",
    "        self.binding_head = nn.Sequential(\n",
    "            nn.Linear(feature_vector_size, feature_vector_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(feature_vector_size // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # The forward pass now just needs to extract features and pass to the head\n",
    "        with torch.no_grad():\n",
    "            # Get predictions and embeddings from the frozen base\n",
    "            base_preds = self.esmdance_base(inputs)\n",
    "            raw_embeddings = self.esmdance_base.esm2(**inputs).last_hidden_state\n",
    "            \n",
    "            # Pooling logic (this can be a helper method if you prefer)\n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "            pooled_embed = (raw_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            \n",
    "            res_keys = self.config['training']['res_feature_idx'].keys()\n",
    "            tensors_to_cat = []\n",
    "            for k in res_keys:\n",
    "                tensor = base_preds[k]\n",
    "                if tensor.dim() == 2:\n",
    "                    tensors_to_cat.append(tensor.unsqueeze(-1))\n",
    "                else:\n",
    "                    tensors_to_cat.append(tensor)\n",
    "            res_features = torch.cat(tensors_to_cat, dim=-1)\n",
    "            pooled_res_features = (res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            \n",
    "            feature_vector = torch.cat([pooled_embed, pooled_res_features], dim=-1)\n",
    "        \n",
    "        # Pass the extracted features through the trainable head\n",
    "        return self.binding_head(feature_vector)\n",
    "\n",
    "# =============================================================================\n",
    "#                            MAIN TRAINING SCRIPT\n",
    "# =============================================================================\n",
    "\n",
    "# --- SETUP ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "full_dataset = BindingDataset(df)\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "# 1. First, perform a \"dry run\" with the base model to get the feature size\n",
    "print(\"Performing dry run to determine feature vector size...\")\n",
    "temp_base_model = ESMwrap(esm2_select='model_35M', model_select='esmdance').to(device)\n",
    "temp_base_model.eval()\n",
    "with torch.no_grad():\n",
    "    dummy_inputs, _ = next(iter(train_loader))\n",
    "    dummy_inputs = {k: v.to(device) for k, v in dummy_inputs.items()}\n",
    "    \n",
    "    # Manually run the feature extraction logic once\n",
    "    base_preds = temp_base_model(dummy_inputs)\n",
    "    raw_embeddings = temp_base_model.esm2(**dummy_inputs).last_hidden_state\n",
    "    attention_mask = dummy_inputs['attention_mask'].unsqueeze(-1)\n",
    "    pooled_embed = (raw_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "    res_keys = base_config['training']['res_feature_idx'].keys()\n",
    "    tensors_to_cat = [base_preds[k].unsqueeze(-1) if base_preds[k].dim() == 2 else base_preds[k] for k in res_keys]\n",
    "    res_features = torch.cat(tensors_to_cat, dim=-1)\n",
    "    pooled_res_features = (res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "    feature_vector = torch.cat([pooled_embed, pooled_res_features], dim=-1)\n",
    "    feature_vector_size = feature_vector.shape[1]\n",
    "    \n",
    "    del temp_base_model # Free up memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Determined concatenated feature vector size: {feature_vector_size}\")\n",
    "\n",
    "# 2. Now, initialize the final model with the correct size\n",
    "model = ESMDanceForBinding(config=base_config, feature_vector_size=feature_vector_size).to(device)\n",
    "\n",
    "# --- LOSS AND OPTIMIZER ---\n",
    "loss_function = nn.MSELoss()\n",
    "# CRITICAL: Pass ONLY the parameters of the new binding_head to the optimizer\n",
    "optimizer = AdamW(model.binding_head.parameters(), lr=1e-4)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# --- TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # =======================================\n",
    "    #               TRAINING\n",
    "    # =======================================\n",
    "    model.train() # Set the binding head to training mode (activates dropout)\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    # --- THIS LOOP WAS MISSING ---\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Training]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device).unsqueeze(1) # Reshape labels for MSELoss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        loss = loss_function(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    # --- END OF MISSING LOOP ---\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # =======================================\n",
    "    #              VALIDATION\n",
    "    # =======================================\n",
    "    model.eval() # Set the binding head to evaluation mode (disables dropout)\n",
    "    total_val_loss = 0\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # --- THIS LOOP WAS MISSING ---\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Validation]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            total_val_loss += loss_function(predictions, labels).item()\n",
    "\n",
    "            # Collect predictions and labels for Spearman correlation\n",
    "            epoch_predictions.append(predictions.cpu().detach())\n",
    "            epoch_labels.append(labels.cpu().detach())\n",
    "        # --- END OF MISSING LOOP ---\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    # Calculate Spearman Correlation\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6915dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing ESM-2 35M for Sequence Classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 7,840,442\n",
      "Starting fine-tuning for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 223/223 [00:06<00:00, 36.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Val]: 100%|██████████| 25/25 [00:00<00:00, 99.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1346 | Spearman Correlation: 0.6494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 223/223 [00:05<00:00, 38.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Val]: 100%|██████████| 25/25 [00:00<00:00, 98.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1107 | Spearman Correlation: 0.7377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 223/223 [00:05<00:00, 38.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Val]: 100%|██████████| 25/25 [00:00<00:00, 96.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0902 | Spearman Correlation: 0.7721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 223/223 [00:05<00:00, 38.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Val]: 100%|██████████| 25/25 [00:00<00:00, 98.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1009 | Spearman Correlation: 0.7779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 223/223 [00:02<00:00, 80.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Val]: 100%|██████████| 25/25 [00:00<00:00, 98.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0812 | Spearman Correlation: 0.7952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmForSequenceClassification\n",
    "\n",
    "# --- SETUP ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- DATA LOADING ---\n",
    "# Load your dataframe with 'aa_sequence' and 'enrichment_score' columns\n",
    "#full_dataset = BindingDataset(df)\n",
    "#\n",
    "#train_size = int(0.9 * len(full_dataset))\n",
    "#val_size = len(full_dataset) - train_size\n",
    "#train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "#\n",
    "## NOTE: The collate function is handled automatically by the DataLoader for this dataset\n",
    "#train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "print(\"Initializing ESM-2 35M for Sequence Classification...\")\n",
    "\n",
    "model = EsmForSequenceClassification.from_pretrained(\n",
    "    \"facebook/esm2_t6_8M_UR50D\",\n",
    "    num_labels=1,                # We are predicting one continuous value.\n",
    "    problem_type=\"regression\"    # Configure the model for regression.\n",
    ").to(device)\n",
    "\n",
    "# --- FREEZE THE BASE MODEL (for fair comparison) ---\n",
    "#print(\"Freezing the ESM-2 base model layers...\")\n",
    "#for name, param in model.named_parameters():\n",
    "#    if name.startswith(\"esm.\"): # This freezes all parameters of the main ESM body\n",
    "#        param.requires_grad = False\n",
    "\n",
    "# --- LOSS AND OPTIMIZER ---\n",
    "# We will let the model calculate its own loss during training, but define it for validation\n",
    "loss_function = nn.MSELoss() \n",
    "\n",
    "# The optimizer will automatically ignore frozen parameters\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Count trainable parameters to confirm the base is frozen\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# --- TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1} [Train]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # The model automatically calculates loss when labels are provided\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch + 1} [Val]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get model predictions (logits)\n",
    "            outputs = model(**inputs)\n",
    "            predictions = outputs.logits\n",
    "            \n",
    "            # Calculate validation loss manually\n",
    "            total_val_loss += loss_function(predictions.squeeze(), labels).item()\n",
    "            \n",
    "            # Collect predictions and labels for Spearman correlation\n",
    "            epoch_predictions.append(predictions.cpu())\n",
    "            epoch_labels.append(labels.cpu())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    # Calculate Spearman Correlation\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1fa98a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing ESM-2 35M for Sequence Classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the ESM-2 base model and unfreezing the final 2 layers...\n",
      "Total trainable parameters: 5,773,441\n",
      "Starting fine-tuning for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 223/223 [00:07<00:00, 30.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Val]: 100%|██████████| 25/25 [00:00<00:00, 44.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1496 | Spearman Correlation: 0.6232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 223/223 [00:07<00:00, 31.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Val]: 100%|██████████| 25/25 [00:00<00:00, 43.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1198 | Spearman Correlation: 0.6874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 223/223 [00:07<00:00, 31.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Val]: 100%|██████████| 25/25 [00:00<00:00, 42.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0966 | Spearman Correlation: 0.7498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 223/223 [00:07<00:00, 31.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Val]: 100%|██████████| 25/25 [00:00<00:00, 43.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1060 | Spearman Correlation: 0.7629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 223/223 [00:04<00:00, 53.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Val]: 100%|██████████| 25/25 [00:00<00:00, 44.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0880 | Spearman Correlation: 0.7812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmForSequenceClassification\n",
    "\n",
    "# --- SETUP ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- DATA LOADING ---\n",
    "# Load your dataframe with 'aa_sequence' and 'enrichment_score' columns\n",
    "#full_dataset = BindingDataset(df)\n",
    "#\n",
    "#train_size = int(0.9 * len(full_dataset))\n",
    "#val_size = len(full_dataset) - train_size\n",
    "#train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "#\n",
    "## NOTE: The collate function is handled automatically by the DataLoader for this dataset\n",
    "#train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "print(\"Initializing ESM-2 35M for Sequence Classification...\")\n",
    "\n",
    "model = EsmForSequenceClassification.from_pretrained(\n",
    "    \"facebook/esm2_t12_35M_UR50D\",\n",
    "    num_labels=1,                # We are predicting one continuous value.\n",
    "    problem_type=\"regression\"    # Configure the model for regression.\n",
    ").to(device)\n",
    "\n",
    "# FREEZE THE BASE, UNFREEZE LAST TWO LAYERS ---\n",
    "print(\"Freezing the ESM-2 base model and unfreezing the final 2 layers...\")\n",
    "\n",
    "# 1. First, freeze all parameters of the entire base model\n",
    "for param in model.esm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. Now, unfreeze only the parameters of the last two transformer layers\n",
    "# The ESM-2 35M model has 12 layers (0-11) in model.esm.encoder.layer\n",
    "num_layers_to_unfreeze = 2\n",
    "for layer in model.esm.encoder.layer[-num_layers_to_unfreeze:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# --- LOSS AND OPTIMIZER ---\n",
    "# We will let the model calculate its own loss during training, but define it for validation\n",
    "loss_function = nn.MSELoss() \n",
    "\n",
    "# The optimizer will automatically ignore frozen parameters\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Count trainable parameters to confirm the base is frozen\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# --- TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1} [Train]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # The model automatically calculates loss when labels are provided\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch + 1} [Val]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get model predictions (logits)\n",
    "            outputs = model(**inputs)\n",
    "            predictions = outputs.logits\n",
    "            \n",
    "            # Calculate validation loss manually\n",
    "            total_val_loss += loss_function(predictions.squeeze(), labels).item()\n",
    "            \n",
    "            # Collect predictions and labels for Spearman correlation\n",
    "            epoch_predictions.append(predictions.cpu())\n",
    "            epoch_labels.append(labels.cpu())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    # Calculate Spearman Correlation\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf9ea931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Performing dry run to determine feature vector size...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determined concatenated feature vector size: 530\n",
      "Initializing base ESMDance model with original 50/13 heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original weights from pretrained_weights/esmdance_update_60000.pt...\n",
      "Initializing trainable binding head with input size 530...\n",
      "Total trainable parameters: 6,869,444\n",
      "Starting fine-tuning for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Training]: 100%|██████████| 223/223 [00:13<00:00, 16.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 0.1981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Validation]: 100%|██████████| 25/25 [00:01<00:00, 16.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss: 0.2119 | Spearman Correlation: 0.5905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Training]: 100%|██████████| 223/223 [00:09<00:00, 24.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Loss: 0.1826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Validation]: 100%|██████████| 25/25 [00:01<00:00, 17.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss: 0.1943 | Spearman Correlation: 0.5917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Training]: 100%|██████████| 223/223 [00:13<00:00, 16.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training Loss: 0.1675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Validation]: 100%|██████████| 25/25 [00:01<00:00, 16.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss: 0.1778 | Spearman Correlation: 0.5977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Training]: 100%|██████████| 223/223 [00:11<00:00, 20.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training Loss: 0.1569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Validation]: 100%|██████████| 25/25 [00:01<00:00, 16.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Loss: 0.1692 | Spearman Correlation: 0.6063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Training]: 100%|██████████| 223/223 [00:13<00:00, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training Loss: 0.1489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Validation]: 100%|██████████| 25/25 [00:01<00:00, 16.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Loss: 0.1643 | Spearman Correlation: 0.6141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "import importlib.util\n",
    "\n",
    "# --- Import the base model class ---\n",
    "from scripts.esmdance_base_model import ESMwrap\n",
    "from scripts.base_config import config as base_config\n",
    "\n",
    "# =============================================================================\n",
    "# 2. NEW MODEL FOR FINE-TUNING\n",
    "# =============================================================================\n",
    "class ESMDanceForBinding(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper model that uses the pre-trained ESMDance as a frozen feature extractor\n",
    "    and adds a new, trainable regression head on top.\n",
    "    \n",
    "    This version initializes all layers in the constructor for robustness.\n",
    "    \"\"\"\n",
    "    # CRITICAL CHANGE: We now pass the feature vector size during initialization\n",
    "    def __init__(self, config, feature_vector_size: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        print(\"Initializing base ESMDance model with original 50/13 heads...\")\n",
    "        self.esmdance_base = ESMwrap(esm2_select='model_35M', model_select='esmdance')\n",
    "        \n",
    "        original_weights_path = 'pretrained_weights/esmdance_update_60000.pt'\n",
    "        print(f\"Loading original weights from {original_weights_path}...\")\n",
    "        self.esmdance_base.load_state_dict(torch.load(original_weights_path, map_location='cpu'))\n",
    "\n",
    "        # Freeze all parameters of the base model\n",
    "        for param in self.esmdance_base.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze final layers of ESMDance\n",
    "        for param in self.esmdance_base.res_pred_nn.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for param in self.esmdance_base.res_transform_nn.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in self.esmdance_base.pair_middle_linear.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in self.esmdance_base.pair_pred_linear.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Unfreeze final layers of ESM2\n",
    "        num_layers_to_unfreeze = 2\n",
    "        for layer in self.esmdance_base.esm2.encoder.layer[-num_layers_to_unfreeze:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # CRITICAL CHANGE: Initialize the binding head immediately in __init__\n",
    "        print(f\"Initializing trainable binding head with input size {feature_vector_size}...\")\n",
    "        self.binding_head = nn.Sequential(\n",
    "            nn.Linear(feature_vector_size, feature_vector_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(feature_vector_size // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # The forward pass now just needs to extract features and pass to the head\n",
    "        with torch.no_grad():\n",
    "            # Get predictions and embeddings from the frozen base\n",
    "            base_preds = self.esmdance_base(inputs)\n",
    "            raw_embeddings = self.esmdance_base.esm2(**inputs).last_hidden_state\n",
    "            \n",
    "            # Pooling logic (this can be a helper method if you prefer)\n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "            pooled_embed = (raw_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            \n",
    "            res_keys = self.config['training']['res_feature_idx'].keys()\n",
    "            tensors_to_cat = []\n",
    "            for k in res_keys:\n",
    "                tensor = base_preds[k]\n",
    "                if tensor.dim() == 2:\n",
    "                    tensors_to_cat.append(tensor.unsqueeze(-1))\n",
    "                else:\n",
    "                    tensors_to_cat.append(tensor)\n",
    "            res_features = torch.cat(tensors_to_cat, dim=-1)\n",
    "            pooled_res_features = (res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            \n",
    "            feature_vector = torch.cat([pooled_embed, pooled_res_features], dim=-1)\n",
    "        \n",
    "        # Pass the extracted features through the trainable head\n",
    "        return self.binding_head(feature_vector)\n",
    "\n",
    "# =============================================================================\n",
    "#                            MAIN TRAINING SCRIPT\n",
    "# =============================================================================\n",
    "\n",
    "# --- SETUP ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "full_dataset = BindingDataset(df)\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "# 1. First, perform a \"dry run\" with the base model to get the feature size\n",
    "print(\"Performing dry run to determine feature vector size...\")\n",
    "temp_base_model = ESMwrap(esm2_select='model_35M', model_select='esmdance').to(device)\n",
    "temp_base_model.eval()\n",
    "with torch.no_grad():\n",
    "    dummy_inputs, _ = next(iter(train_loader))\n",
    "    dummy_inputs = {k: v.to(device) for k, v in dummy_inputs.items()}\n",
    "    \n",
    "    # Manually run the feature extraction logic once\n",
    "    base_preds = temp_base_model(dummy_inputs)\n",
    "    raw_embeddings = temp_base_model.esm2(**dummy_inputs).last_hidden_state\n",
    "    attention_mask = dummy_inputs['attention_mask'].unsqueeze(-1)\n",
    "    pooled_embed = (raw_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "    res_keys = base_config['training']['res_feature_idx'].keys()\n",
    "    tensors_to_cat = [base_preds[k].unsqueeze(-1) if base_preds[k].dim() == 2 else base_preds[k] for k in res_keys]\n",
    "    res_features = torch.cat(tensors_to_cat, dim=-1)\n",
    "    pooled_res_features = (res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "    feature_vector = torch.cat([pooled_embed, pooled_res_features], dim=-1)\n",
    "    feature_vector_size = feature_vector.shape[1]\n",
    "    \n",
    "    del temp_base_model # Free up memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Determined concatenated feature vector size: {feature_vector_size}\")\n",
    "\n",
    "# 2. Now, initialize the final model with the correct size\n",
    "model = ESMDanceForBinding(config=base_config, feature_vector_size=feature_vector_size).to(device)\n",
    "\n",
    "# --- LOSS AND OPTIMIZER ---\n",
    "loss_function = nn.MSELoss()\n",
    "# CRITICAL: Pass ONLY the parameters of the new binding_head to the optimizer\n",
    "optimizer = AdamW(model.binding_head.parameters(), lr=1e-4)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# --- TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # =======================================\n",
    "    #               TRAINING\n",
    "    # =======================================\n",
    "    model.train() # Set the binding head to training mode (activates dropout)\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    # --- THIS LOOP WAS MISSING ---\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Training]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device).unsqueeze(1) # Reshape labels for MSELoss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        loss = loss_function(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    # --- END OF MISSING LOOP ---\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # =======================================\n",
    "    #              VALIDATION\n",
    "    # =======================================\n",
    "    model.eval() # Set the binding head to evaluation mode (disables dropout)\n",
    "    total_val_loss = 0\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # --- THIS LOOP WAS MISSING ---\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Validation]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            total_val_loss += loss_function(predictions, labels).item()\n",
    "\n",
    "            # Collect predictions and labels for Spearman correlation\n",
    "            epoch_predictions.append(predictions.cpu().detach())\n",
    "            epoch_labels.append(labels.cpu().detach())\n",
    "        # --- END OF MISSING LOOP ---\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    # Calculate Spearman Correlation\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ba48a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
