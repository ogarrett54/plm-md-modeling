{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27bbc2b0",
   "metadata": {},
   "source": [
    "# Fine-tuning ESMDance Models on Yeast Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321c6dc",
   "metadata": {},
   "source": [
    "Objectives\n",
    "- Fine-tune ESMDance base model and mutant NMA expert model on yeast data.\n",
    "- Compare to ESM2 base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e0b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aeef79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_sequence</th>\n",
       "      <th>enrichment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>1.468796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GLKRIIVIKVAREGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.415944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GLKRIIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.389615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>1.359651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...</td>\n",
       "      <td>1.343857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3955</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>-1.041749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3956</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEA...</td>\n",
       "      <td>-1.041749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3957</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...</td>\n",
       "      <td>-1.057543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3958</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKTEV...</td>\n",
       "      <td>-1.057543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3959</th>\n",
       "      <td>GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKTEV...</td>\n",
       "      <td>-1.057543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3960 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            aa_sequence  enrichment_score\n",
       "0     GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...          1.468796\n",
       "1     GLKRIIVIKVAREGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.415944\n",
       "2     GLKRIIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.389615\n",
       "3     GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...          1.359651\n",
       "4     GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRGKIEV...          1.343857\n",
       "...                                                 ...               ...\n",
       "3955  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...         -1.041749\n",
       "3956  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEA...         -1.041749\n",
       "3957  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKIEV...         -1.057543\n",
       "3958  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKTEV...         -1.057543\n",
       "3959  GLKQKIVIKVAMEGNNCRSKAMALVASTGGVDSVALVGDLRDKTEV...         -1.057543\n",
       "\n",
       "[3960 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir = Path('yeast_data')\n",
    "df = pd.read_csv(str(input_dir / 'avrpikC_full.csv'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58e0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BindingDataset(Dataset):\n",
    "    \"\"\"Dataset for the final binding prediction task.\"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        # Ensure the columns exist\n",
    "        assert 'aa_sequence' in self.df.columns, \"DataFrame must have 'aa_sequence' column.\"\n",
    "        assert 'enrichment_score' in self.df.columns, \"DataFrame must have 'enrichment_score' column.\"\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.df.iloc[idx]['aa_sequence']\n",
    "        # The label is a single float value\n",
    "        label = torch.tensor(self.df.iloc[idx]['enrichment_score'], dtype=torch.float)\n",
    "\n",
    "        # Tokenize the sequence\n",
    "        tokenized_output = self.tokenizer(\n",
    "            sequence,\n",
    "            max_length=160, # Use a fixed max length. 158 residues plus 2 extra tokens\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Return input_ids: attention masks, removing batch dimension\n",
    "        inputs = {key: val.squeeze(0) for key, val in tokenized_output.items()}\n",
    "        \n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a9560",
   "metadata": {},
   "source": [
    "## Approach 1: Frozen ESM and ESMDance\n",
    "Use base ESM, ESMDance, and fine-tuned NMA as a feature extractor for simple linear regression head. This method pools each of the features into a single numerical representation, which doesn't actually make that much sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca63438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from scripts.esmdance_flex_model import ESMwrap # Customized ESMDance model definition for dynamic config usage\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, original_model_config, nma_model_config, nma_model_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Instantiate the original ESMDance with its 50/13 res/pair features config\n",
    "        print(\"Initializing original ESMDance model...\")\n",
    "        self.original_esmdance = ESMwrap(model_config=original_model_config)\n",
    "        self.original_esmdance = self.original_esmdance.from_pretrained(\"ChaoHou/ESMDance\", model_config=original_model_config)\n",
    "\n",
    "        # Instantiate the NMA-tuned model with the 3/3 res/pair features config\n",
    "        print(f\"Initializing custom NMA-tuned model from {nma_model_path}...\")\n",
    "        self.nma_esmdance = ESMwrap(model_config=nma_model_config)\n",
    "        self.nma_esmdance.load_state_dict(torch.load(nma_model_path, map_location='cpu'))\n",
    "\n",
    "        # Freeze parameters\n",
    "        print(\"Freezing all parameters in the feature extractor...\")\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.eval()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            md_preds = self.original_esmdance(inputs) # Get features from original ESMDance\n",
    "            nma_preds = self.nma_esmdance(inputs) # Get features from the mutant, fine-tuned ESMDance\n",
    "            raw_embeddings = self.original_esmdance.esm2(**inputs).last_hidden_state # Get features from ESM2\n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1) # Add dimension to allow matrix multiplication with raw embeddings\n",
    "            \n",
    "            # Pool raw embeddings, averaging across residues to give each sequence a single representation\n",
    "            pooled_embed = (raw_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "\n",
    "            # Gather and unify dimensions for all MD residue features\n",
    "            original_res_keys = self.original_esmdance.config['training']['res_feature_idx'].keys()\n",
    "            md_tensors_to_cat = []\n",
    "            for k in original_res_keys:\n",
    "                tensor = md_preds[k]\n",
    "                if tensor.dim() == 2:\n",
    "                    # If tensor is 2D (e.g., shape [B, L]), add a feature dimension\n",
    "                    md_tensors_to_cat.append(tensor.unsqueeze(-1))\n",
    "                else:\n",
    "                    # If tensor is already 3D (e.g., shape [B, L, F]), add it as is\n",
    "                    md_tensors_to_cat.append(tensor)\n",
    "            md_res_features = torch.cat(md_tensors_to_cat, dim=-1)\n",
    "\n",
    "            # Gather and unify dimensions for all 3 NMA residue features\n",
    "            nma_res_keys = self.nma_esmdance.config['training']['res_feature_idx'].keys()\n",
    "            nma_tensors_to_cat = []\n",
    "            for k in nma_res_keys:\n",
    "                tensor = nma_preds[k]\n",
    "                if tensor.dim() == 2:\n",
    "                    nma_tensors_to_cat.append(tensor.unsqueeze(-1))\n",
    "                else:\n",
    "                    nma_tensors_to_cat.append(tensor)\n",
    "            nma_res_features = torch.cat(nma_tensors_to_cat, dim=-1)\n",
    "\n",
    "            # Pool the correctly shaped features\n",
    "            pooled_md_res = (md_res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            pooled_nma_res = (nma_res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            \n",
    "            # Concatenate all features into one vector\n",
    "            final_feature_vector = torch.cat([pooled_embed, pooled_md_res, pooled_nma_res], dim=-1)\n",
    "            \n",
    "        return final_feature_vector\n",
    "\n",
    "\n",
    "class BindingHead(nn.Module):\n",
    "    \"\"\"\n",
    "    The small regression head that we will train.\n",
    "    It takes the concatenated feature vector as input.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features):\n",
    "        super().__init__()\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(input_features, input_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(input_features // 2, 1) # Output a single value for enrichment\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.regression_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "626816ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing original ESMDance model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing custom NMA-tuned model from models/esmdance-mutant-nma-fine-tuned/esmdance_fine-tuned_with_nma_data.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing all parameters in the feature extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining feature vector size...\n",
      "Concatenated feature vector size: 533\n",
      "Starting training of the binding head for 5 epochs...\n",
      "\n",
      "Epoch 1/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:28<00:00,  7.94it/s]\n",
      "[Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2002 | Validation Loss: 0.1825 | Spearman Correlation: 0.5288\n",
      "\n",
      "Epoch 2/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:27<00:00,  8.11it/s]\n",
      "[Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1828 | Validation Loss: 0.1616 | Spearman Correlation: 0.5324\n",
      "\n",
      "Epoch 3/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:27<00:00,  8.08it/s]\n",
      "[Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1684 | Validation Loss: 0.1478 | Spearman Correlation: 0.5386\n",
      "\n",
      "Epoch 4/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:28<00:00,  7.92it/s]\n",
      "[Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1586 | Validation Loss: 0.1383 | Spearman Correlation: 0.5450\n",
      "\n",
      "Epoch 5/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:32<00:00,  6.97it/s]\n",
      "[Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1504 | Validation Loss: 0.1325 | Spearman Correlation: 0.5524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from scripts.base_config import config as base_config\n",
    "from scripts.nma_finetuned_config import config as nma_config\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "# --- SETUP ---\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- DATA LOADING ---\n",
    "# Load your dataframe with 'aa_sequence' and 'enrichment_score' columns\n",
    "full_dataset = BindingDataset(df)\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "nma_model_path = 'models/esmdance-mutant-nma-fine-tuned/esmdance_fine-tuned_with_nma_data.pth' # Path to your NMA-tuned model\n",
    "\n",
    "# Initialize the frozen feature extractor and the trainable binding head\n",
    "feature_extractor = FeatureExtractor(original_model_config=base_config, \n",
    "                                     nma_model_config=nma_config, \n",
    "                                     nma_model_path=nma_model_path).to(device)\n",
    "\n",
    "# Determine the input size for the binding head after one forward pass\n",
    "print(\"Determining feature vector size...\")\n",
    "with torch.no_grad():\n",
    "    dummy_inputs, _ = next(iter(train_loader))\n",
    "    dummy_inputs = {k: v.to(device) for k,v in dummy_inputs.items()}\n",
    "    dummy_feature_vector = feature_extractor(dummy_inputs,)\n",
    "    feature_vector_size = dummy_feature_vector.shape[1]\n",
    "\n",
    "print(f\"Concatenated feature vector size: {feature_vector_size}\")\n",
    "binding_head = BindingHead(feature_vector_size).to(device)\n",
    "\n",
    "# --- LOSS AND OPTIMIZER ---\n",
    "# MSE is a good loss function for regression tasks like enrichment scores\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Pass ONLY the parameters of the binding_head to the optimizer\n",
    "optimizer = AdamW(binding_head.parameters(), lr=1e-4)\n",
    "\n",
    "# --- TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting training of the binding head for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{num_epochs}\\n----------------------------')\n",
    "    # =======================================\n",
    "    #               TRAINING\n",
    "    # =======================================\n",
    "    binding_head.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"[Train]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device).unsqueeze(1) # Reshape labels for MSELoss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        feature_vector = feature_extractor(inputs)\n",
    "        predictions = binding_head(feature_vector)\n",
    "        \n",
    "        loss = loss_function(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # =======================================\n",
    "    #              VALIDATION\n",
    "    # =======================================\n",
    "    binding_head.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"[Val]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            feature_vector = feature_extractor(inputs)\n",
    "            predictions = binding_head(feature_vector)\n",
    "            \n",
    "            total_val_loss += loss_function(predictions, labels).item()\n",
    "            epoch_predictions.append(predictions.cpu().detach())\n",
    "            epoch_labels.append(labels.cpu().detach())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    # Concatenate all batch tensors into single, large tensors\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    \n",
    "    # Calculate Spearman's rank correlation coefficient\n",
    "    # spearmanr returns two values: the correlation and the p-value\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    # --- Print metrics ---\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc336794",
   "metadata": {},
   "source": [
    "## Baseline 1.1: 35M ESM2-only feature extractor\n",
    "For a simple comparison, what happens when you only use ESM2 as the feature extractor for the binding head above. This uses the special class token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12378a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing ESM-2 35M for Sequence Classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the ESM-2 base model layers...\n",
      "Total trainable parameters: 231,361\n",
      "Starting fine-tuning for 5 epochs...\n",
      "\n",
      "Epoch 1/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:05<00:00, 37.48it/s] \n",
      "[Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 29.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2091 | Validation Loss: 0.1943 | Spearman Correlation: 0.5128\n",
      "\n",
      "Epoch 2/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:08<00:00, 26.61it/s]\n",
      "[Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 26.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2015 | Validation Loss: 0.1979 | Spearman Correlation: 0.5311\n",
      "\n",
      "Epoch 3/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:08<00:00, 27.20it/s]\n",
      "[Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 25.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1930 | Validation Loss: 0.1779 | Spearman Correlation: 0.5412\n",
      "\n",
      "Epoch 4/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:05<00:00, 42.07it/s]\n",
      "[Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 28.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1841 | Validation Loss: 0.1641 | Spearman Correlation: 0.5499\n",
      "\n",
      "Epoch 5/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:08<00:00, 27.57it/s]\n",
      "[Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 26.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1734 | Validation Loss: 0.1513 | Spearman Correlation: 0.5563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmForSequenceClassification\n",
    "\n",
    "# --- SETUP ---\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "print(\"Initializing ESM-2 35M for Sequence Classification...\")\n",
    "\n",
    "model = EsmForSequenceClassification.from_pretrained(\n",
    "    \"facebook/esm2_t12_35M_UR50D\",\n",
    "    num_labels=1,                # We are predicting one continuous value.\n",
    "    problem_type=\"regression\"    # Configure the model for regression.\n",
    ").to(device)\n",
    "\n",
    "# --- FREEZE THE BASE MODEL (for fair comparison) ---\n",
    "print(\"Freezing the ESM-2 base model layers...\")\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"esm.\"): # This freezes all parameters of the main ESM body\n",
    "        param.requires_grad = False\n",
    "\n",
    "# --- LOSS AND OPTIMIZER ---\n",
    "# We will let the model calculate its own loss during training, but define it for validation\n",
    "loss_function = nn.MSELoss() \n",
    "\n",
    "# The optimizer will automatically ignore frozen parameters\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Count trainable parameters to confirm the base is frozen\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# --- TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{num_epochs}\\n----------------------------')\n",
    "    # =======================================\n",
    "    #               TRAINING\n",
    "    # =======================================\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"[Train]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # The model automatically calculates loss when labels are provided\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # =======================================\n",
    "    #              VALIDATION\n",
    "    # =======================================\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"[Val]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get model predictions (logits)\n",
    "            outputs = model(**inputs)\n",
    "            predictions = outputs.logits\n",
    "            \n",
    "            # Calculate validation loss manually\n",
    "            total_val_loss += loss_function(predictions.squeeze(), labels).item()\n",
    "            \n",
    "            # Collect predictions and labels for Spearman correlation\n",
    "            epoch_predictions.append(predictions.cpu())\n",
    "            epoch_labels.append(labels.cpu())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    # Calculate Spearman Correlation\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    print(f\"Training Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296969d4",
   "metadata": {},
   "source": [
    "## Baseline 1.2: Using ESMDance only as a feature extractor\n",
    "For another comparison to approach one, I'm using just ESMDance predictions as a feature extractor. Essentially using predicted molecular dynamics features. I should revisit this, as pooling these features for the binding head makes zero sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d22e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Performing dry run to determine feature vector size...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determined concatenated feature vector size: 530\n",
      "Initializing base ESMDance model with original 50/13 heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original weights from pretrained_weights/esmdance_update_60000.pt...\n",
      "Initializing trainable binding head with input size 530...\n",
      "Total trainable parameters: 140,981\n",
      "Starting fine-tuning for 5 epochs...\n",
      "\n",
      "Epoch 1/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:16<00:00, 13.33it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:02<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1982 | Validation Loss: 0.1783 | Spearman Correlation: 0.5279\n",
      "\n",
      "Epoch 2/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:19<00:00, 11.23it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:02<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1818 | Validation Loss: 0.1612 | Spearman Correlation: 0.5305\n",
      "\n",
      "Epoch 3/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:16<00:00, 13.38it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:02<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1690 | Validation Loss: 0.1480 | Spearman Correlation: 0.5356\n",
      "\n",
      "Epoch 4/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:16<00:00, 13.49it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1571 | Validation Loss: 0.1409 | Spearman Correlation: 0.5436\n",
      "\n",
      "Epoch 5/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:19<00:00, 11.61it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:02<00:00, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1514 | Validation Loss: 0.1326 | Spearman Correlation: 0.5535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "import importlib.util\n",
    "\n",
    "# --- Import the base model class ---\n",
    "from scripts.esmdance_base_model import ESMwrap\n",
    "from scripts.base_config import config as base_config\n",
    "\n",
    "class ESMDanceForBinding(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper model that uses the pre-trained ESMDance as a frozen feature extractor\n",
    "    and adds a new, trainable regression head on top.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, feature_vector_size: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        print(\"Initializing base ESMDance model with original 50/13 heads...\")\n",
    "        self.esmdance_base = ESMwrap(esm2_select='model_35M', model_select='esmdance')\n",
    "        \n",
    "        original_weights_path = 'pretrained_weights/esmdance_update_60000.pt'\n",
    "        print(f\"Loading original weights from {original_weights_path}...\")\n",
    "        self.esmdance_base.load_state_dict(torch.load(original_weights_path, map_location='cpu'))\n",
    "\n",
    "        # Freeze all parameters of the base model\n",
    "        for param in self.esmdance_base.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Initialize binding head\n",
    "        print(f\"Initializing trainable binding head with input size {feature_vector_size}...\")\n",
    "        self.binding_head = nn.Sequential(\n",
    "            nn.Linear(feature_vector_size, feature_vector_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(feature_vector_size // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # The forward pass now just needs to extract features and pass to the head\n",
    "        with torch.no_grad():\n",
    "            # Get predictions and embeddings from the frozen base\n",
    "            base_preds = self.esmdance_base(inputs)\n",
    "            raw_embeddings = self.esmdance_base.esm2(**inputs).last_hidden_state\n",
    "            \n",
    "            # Pooling logic (this can be a helper method if you prefer)\n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "            pooled_embed = (raw_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            \n",
    "            res_keys = self.config['training']['res_feature_idx'].keys()\n",
    "            tensors_to_cat = []\n",
    "            for k in res_keys:\n",
    "                tensor = base_preds[k]\n",
    "                if tensor.dim() == 2:\n",
    "                    tensors_to_cat.append(tensor.unsqueeze(-1))\n",
    "                else:\n",
    "                    tensors_to_cat.append(tensor)\n",
    "            res_features = torch.cat(tensors_to_cat, dim=-1)\n",
    "            pooled_res_features = (res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            \n",
    "            feature_vector = torch.cat([pooled_embed, pooled_res_features], dim=-1)\n",
    "        \n",
    "        # Pass the extracted features through the trainable head\n",
    "        return self.binding_head(feature_vector)\n",
    "\n",
    "# --- SETUP ---\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "full_dataset = BindingDataset(df)\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "# 1. Perform a \"dry run\" with the base model to get the feature size\n",
    "print(\"Performing dry run to determine feature vector size...\")\n",
    "temp_base_model = ESMwrap(esm2_select='model_35M', model_select='esmdance').to(device)\n",
    "temp_base_model.eval()\n",
    "with torch.no_grad():\n",
    "    dummy_inputs, _ = next(iter(train_loader))\n",
    "    dummy_inputs = {k: v.to(device) for k, v in dummy_inputs.items()}\n",
    "    \n",
    "    # Manually run the feature extraction logic once\n",
    "    base_preds = temp_base_model(dummy_inputs)\n",
    "    raw_embeddings = temp_base_model.esm2(**dummy_inputs).last_hidden_state\n",
    "    attention_mask = dummy_inputs['attention_mask'].unsqueeze(-1)\n",
    "    pooled_embed = (raw_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "    res_keys = base_config['training']['res_feature_idx'].keys()\n",
    "    tensors_to_cat = [base_preds[k].unsqueeze(-1) if base_preds[k].dim() == 2 else base_preds[k] for k in res_keys]\n",
    "    res_features = torch.cat(tensors_to_cat, dim=-1)\n",
    "    pooled_res_features = (res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "    feature_vector = torch.cat([pooled_embed, pooled_res_features], dim=-1)\n",
    "    feature_vector_size = feature_vector.shape[1]\n",
    "    \n",
    "    del temp_base_model # Free up memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Determined concatenated feature vector size: {feature_vector_size}\")\n",
    "\n",
    "# Now we can initialize the final model with the correct size\n",
    "model = ESMDanceForBinding(config=base_config, feature_vector_size=feature_vector_size).to(device)\n",
    "\n",
    "# --- LOSS AND OPTIMIZER ---\n",
    "loss_function = nn.MSELoss()\n",
    "# Pass the parameters of the new binding_head to the optimizer\n",
    "optimizer = AdamW(model.binding_head.parameters(), lr=1e-4)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# --- TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{num_epochs}\\n----------------------------')\n",
    "    # =======================================\n",
    "    #               TRAINING\n",
    "    # =======================================\n",
    "    model.train() # Set the binding head to training mode (activates dropout)\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"[Training]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device).unsqueeze(1) # Reshape labels for MSELoss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        loss = loss_function(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # =======================================\n",
    "    #              VALIDATION\n",
    "    # =======================================\n",
    "    model.eval() # Set the binding head to evaluation mode (disables dropout)\n",
    "    total_val_loss = 0\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"[Validation]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            total_val_loss += loss_function(predictions, labels).item()\n",
    "\n",
    "            # Collect predictions and labels for Spearman correlation\n",
    "            epoch_predictions.append(predictions.cpu().detach())\n",
    "            epoch_labels.append(labels.cpu().detach())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    # Calculate Spearman Correlation\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    print(f\"Training Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c62b56",
   "metadata": {},
   "source": [
    "## Baseline 1.3: Fine-tuning the full ESM2 8M model\n",
    "This is the default method that Alexander used, though he used the huggingface transformer library, so results are slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6915dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing ESM-2 35M for Sequence Classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 7,840,442\n",
      "Starting fine-tuning for 5 epochs...\n",
      "\n",
      "Epoch 1/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:11<00:00, 18.69it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 55.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1978 | Validation Loss: 0.1723 | Spearman Correlation: 0.5596\n",
      "\n",
      "Epoch 2/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:14<00:00, 15.11it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 41.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1420 | Validation Loss: 0.1197 | Spearman Correlation: 0.6244\n",
      "\n",
      "Epoch 3/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:11<00:00, 19.16it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 52.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1210 | Validation Loss: 0.1192 | Spearman Correlation: 0.6939\n",
      "\n",
      "Epoch 4/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:15<00:00, 14.38it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 33.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1044 | Validation Loss: 0.0923 | Spearman Correlation: 0.7229\n",
      "\n",
      "Epoch 5/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:18<00:00, 11.80it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 48.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0966 | Validation Loss: 0.0859 | Spearman Correlation: 0.7586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmForSequenceClassification\n",
    "\n",
    "# --- SETUP ---\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "print(\"Initializing ESM-2 35M for Sequence Classification...\")\n",
    "\n",
    "model = EsmForSequenceClassification.from_pretrained(\n",
    "    \"facebook/esm2_t6_8M_UR50D\",\n",
    "    num_labels=1,                # We are predicting one continuous value.\n",
    "    problem_type=\"regression\"    # Configure the model for regression.\n",
    ").to(device)\n",
    "\n",
    "# --- LOSS AND OPTIMIZER ---\n",
    "# We will let the model calculate its own loss during training, but define it for validation\n",
    "loss_function = nn.MSELoss() \n",
    "\n",
    "# The optimizer will automatically ignore frozen parameters\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Count trainable parameters to confirm the base is frozen\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# --- TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{num_epochs}\\n----------------------------')\n",
    "    # =======================================\n",
    "    #               TRAINING\n",
    "    # =======================================\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"[Training]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # The model automatically calculates loss when labels are provided\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # =======================================\n",
    "    #              VALIDATION\n",
    "    # =======================================\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"[Validation]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get model predictions (logits)\n",
    "            outputs = model(**inputs)\n",
    "            predictions = outputs.logits\n",
    "            \n",
    "            # Calculate validation loss manually\n",
    "            total_val_loss += loss_function(predictions.squeeze(), labels).item()\n",
    "            \n",
    "            # Collect predictions and labels for Spearman correlation\n",
    "            epoch_predictions.append(predictions.cpu())\n",
    "            epoch_labels.append(labels.cpu())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    # Calculate Spearman Correlation\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    print(f\"Training Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f080b92",
   "metadata": {},
   "source": [
    "## Baseline 1.4: 35M ESM2 Model with Last Two Layers Unfrozen\n",
    "This is to test another strategy for fine-tuning that was detailed in the ESMEffect paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa98a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing ESM-2 35M for Sequence Classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the ESM-2 base model and unfreezing the final 2 layers...\n",
      "Total trainable parameters: 5,773,441\n",
      "Starting fine-tuning for 5 epochs...\n",
      "\n",
      "Epoch 1/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:13<00:00, 16.42it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 19.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1906 | Validation Loss: 0.1245 | Spearman Correlation: 0.6330\n",
      "\n",
      "Epoch 2/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:11<00:00, 18.79it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 30.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1268 | Validation Loss: 0.1068 | Spearman Correlation: 0.7100\n",
      "\n",
      "Epoch 3/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:10<00:00, 20.96it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 28.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1140 | Validation Loss: 0.0906 | Spearman Correlation: 0.7507\n",
      "\n",
      "Epoch 4/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:08<00:00, 27.46it/s] \n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1022 | Validation Loss: 0.0885 | Spearman Correlation: 0.7503\n",
      "\n",
      "Epoch 5/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:10<00:00, 20.68it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 28.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0930 | Validation Loss: 0.1062 | Spearman Correlation: 0.7602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmForSequenceClassification\n",
    "\n",
    "# --- SETUP ---\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "print(\"Initializing ESM-2 35M for Sequence Classification...\")\n",
    "\n",
    "model = EsmForSequenceClassification.from_pretrained(\n",
    "    \"facebook/esm2_t12_35M_UR50D\",\n",
    "    num_labels=1,                # We are predicting one continuous value.\n",
    "    problem_type=\"regression\"    # Configure the model for regression.\n",
    ").to(device)\n",
    "\n",
    "# FREEZE THE BASE, UNFREEZE LAST TWO LAYERS ---\n",
    "print(\"Freezing the ESM-2 base model and unfreezing the final 2 layers...\")\n",
    "\n",
    "# 1. First, freeze all parameters of the entire base model\n",
    "for param in model.esm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. Now, unfreeze only the parameters of the last two transformer layers\n",
    "# The ESM-2 35M model has 12 layers (0-11) in model.esm.encoder.layer\n",
    "num_layers_to_unfreeze = 2\n",
    "for layer in model.esm.encoder.layer[-num_layers_to_unfreeze:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# --- LOSS AND OPTIMIZER ---\n",
    "# We will let the model calculate its own loss during training, but define it for validation\n",
    "loss_function = nn.MSELoss() \n",
    "\n",
    "# The optimizer will automatically ignore frozen parameters\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Count trainable parameters to confirm the base is frozen\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# --- TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{num_epochs}\\n----------------------------')\n",
    "    # =======================================\n",
    "    #               TRAINING\n",
    "    # =======================================\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"[Training]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # The model automatically calculates loss when labels are provided\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # =======================================\n",
    "    #              VALIDATION\n",
    "    # =======================================\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"[Validation]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get model predictions (logits)\n",
    "            outputs = model(**inputs)\n",
    "            predictions = outputs.logits\n",
    "            \n",
    "            # Calculate validation loss manually\n",
    "            total_val_loss += loss_function(predictions.squeeze(), labels).item()\n",
    "            \n",
    "            # Collect predictions and labels for Spearman correlation\n",
    "            epoch_predictions.append(predictions.cpu())\n",
    "            epoch_labels.append(labels.cpu())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    # Calculate Spearman Correlation\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    print(f\"Training Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a8b02c",
   "metadata": {},
   "source": [
    "## Baseline 1.5: ESMDance with Extra Layers Unfrozen\n",
    "This model unfreezes all of the layers added by ESMDance to ESM2, as well the final two layers of ESM2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9ea931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Performing dry run to determine feature vector size...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determined concatenated feature vector size: 530\n",
      "Initializing base ESMDance model with original 50/13 heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original weights from pretrained_weights/esmdance_update_60000.pt...\n",
      "Initializing trainable binding head with input size 530...\n",
      "Total trainable parameters: 6,869,444\n",
      "Starting fine-tuning for 5 epochs...\n",
      "\n",
      "Epoch 1/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:13<00:00, 17.09it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 17.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1982 | Validation Loss: 0.1783 | Spearman Correlation: 0.5279\n",
      "\n",
      "Epoch 2/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:10<00:00, 20.95it/s] \n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 17.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1818 | Validation Loss: 0.1612 | Spearman Correlation: 0.5305\n",
      "\n",
      "Epoch 3/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:13<00:00, 16.87it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 17.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1690 | Validation Loss: 0.1480 | Spearman Correlation: 0.5356\n",
      "\n",
      "Epoch 4/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:10<00:00, 21.00it/s] \n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 17.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1571 | Validation Loss: 0.1409 | Spearman Correlation: 0.5436\n",
      "\n",
      "Epoch 5/5\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 223/223 [00:13<00:00, 17.01it/s]\n",
      "[Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:01<00:00, 17.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1514 | Validation Loss: 0.1326 | Spearman Correlation: 0.5535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "import importlib.util\n",
    "\n",
    "# --- Import the base model class ---\n",
    "from scripts.esmdance_base_model import ESMwrap\n",
    "from scripts.base_config import config as base_config\n",
    "\n",
    "class ESMDanceForBinding(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper model that uses the pre-trained ESMDance as a frozen feature extractor\n",
    "    and adds a new, trainable regression head on top.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, feature_vector_size: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        print(\"Initializing base ESMDance model with original 50/13 heads...\")\n",
    "        self.esmdance_base = ESMwrap(esm2_select='model_35M', model_select='esmdance')\n",
    "        \n",
    "        original_weights_path = 'pretrained_weights/esmdance_update_60000.pt'\n",
    "        print(f\"Loading original weights from {original_weights_path}...\")\n",
    "        self.esmdance_base.load_state_dict(torch.load(original_weights_path, map_location='cpu'))\n",
    "\n",
    "        # Freeze all parameters of the base model\n",
    "        for param in self.esmdance_base.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze final layers of ESMDance\n",
    "        for param in self.esmdance_base.res_pred_nn.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for param in self.esmdance_base.res_transform_nn.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in self.esmdance_base.pair_middle_linear.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in self.esmdance_base.pair_pred_linear.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Unfreeze final layers of ESM2\n",
    "        num_layers_to_unfreeze = 2\n",
    "        for layer in self.esmdance_base.esm2.encoder.layer[-num_layers_to_unfreeze:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Initialize the binding head\n",
    "        print(f\"Initializing trainable binding head with input size {feature_vector_size}...\")\n",
    "        self.binding_head = nn.Sequential(\n",
    "            nn.Linear(feature_vector_size, feature_vector_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(feature_vector_size // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # The forward pass now just needs to extract features and pass to the head\n",
    "        with torch.no_grad():\n",
    "            # Get predictions and embeddings from the frozen base\n",
    "            base_preds = self.esmdance_base(inputs)\n",
    "            raw_embeddings = self.esmdance_base.esm2(**inputs).last_hidden_state\n",
    "            \n",
    "            # Pooling logic (this can be a helper method if you prefer)\n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "            pooled_embed = (raw_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            \n",
    "            res_keys = self.config['training']['res_feature_idx'].keys()\n",
    "            tensors_to_cat = []\n",
    "            for k in res_keys:\n",
    "                tensor = base_preds[k]\n",
    "                if tensor.dim() == 2:\n",
    "                    tensors_to_cat.append(tensor.unsqueeze(-1))\n",
    "                else:\n",
    "                    tensors_to_cat.append(tensor)\n",
    "            res_features = torch.cat(tensors_to_cat, dim=-1)\n",
    "            pooled_res_features = (res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            \n",
    "            feature_vector = torch.cat([pooled_embed, pooled_res_features], dim=-1)\n",
    "        \n",
    "        # Pass the extracted features through the trainable head\n",
    "        return self.binding_head(feature_vector)\n",
    "\n",
    "# =============================================================================\n",
    "#                            MAIN TRAINING SCRIPT\n",
    "# =============================================================================\n",
    "\n",
    "# --- SETUP ---\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "full_dataset = BindingDataset(df)\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# --- MODEL INITIALIZATION ---\n",
    "# 1. Perform a \"dry run\" with the base model to get the feature size\n",
    "print(\"Performing dry run to determine feature vector size...\")\n",
    "temp_base_model = ESMwrap(esm2_select='model_35M', model_select='esmdance').to(device)\n",
    "temp_base_model.eval()\n",
    "with torch.no_grad():\n",
    "    dummy_inputs, _ = next(iter(train_loader))\n",
    "    dummy_inputs = {k: v.to(device) for k, v in dummy_inputs.items()}\n",
    "    \n",
    "    # Manually run the feature extraction logic once\n",
    "    base_preds = temp_base_model(dummy_inputs)\n",
    "    raw_embeddings = temp_base_model.esm2(**dummy_inputs).last_hidden_state\n",
    "    attention_mask = dummy_inputs['attention_mask'].unsqueeze(-1)\n",
    "    pooled_embed = (raw_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "    res_keys = base_config['training']['res_feature_idx'].keys()\n",
    "    tensors_to_cat = [base_preds[k].unsqueeze(-1) if base_preds[k].dim() == 2 else base_preds[k] for k in res_keys]\n",
    "    res_features = torch.cat(tensors_to_cat, dim=-1)\n",
    "    pooled_res_features = (res_features * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "    feature_vector = torch.cat([pooled_embed, pooled_res_features], dim=-1)\n",
    "    feature_vector_size = feature_vector.shape[1]\n",
    "    \n",
    "    del temp_base_model # Free up memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Determined concatenated feature vector size: {feature_vector_size}\")\n",
    "\n",
    "# Initialize the final model with the correct size\n",
    "model = ESMDanceForBinding(config=base_config, feature_vector_size=feature_vector_size).to(device)\n",
    "\n",
    "# --- LOSS AND OPTIMIZER ---\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# --- TRAINING & VALIDATION LOOP ---\n",
    "num_epochs = 5\n",
    "print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{num_epochs}\\n----------------------------')\n",
    "    # =======================================\n",
    "    #               TRAINING\n",
    "    # =======================================\n",
    "    model.train() # Set the binding head to training mode (activates dropout)\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"[Training]\"):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device).unsqueeze(1) # Reshape labels for MSELoss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        loss = loss_function(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # =======================================\n",
    "    #              VALIDATION\n",
    "    # =======================================\n",
    "    model.eval() # Set the binding head to evaluation mode (disables dropout)\n",
    "    total_val_loss = 0\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"[Validation]\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            total_val_loss += loss_function(predictions, labels).item()\n",
    "\n",
    "            # Collect predictions and labels for Spearman correlation\n",
    "            epoch_predictions.append(predictions.cpu().detach())\n",
    "            epoch_labels.append(labels.cpu().detach())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    # Calculate Spearman Correlation\n",
    "    all_predictions = torch.cat(epoch_predictions).numpy().flatten()\n",
    "    all_labels = torch.cat(epoch_labels).numpy().flatten()\n",
    "    spearman_corr, p_value = spearmanr(all_predictions, all_labels)\n",
    "    \n",
    "    print(f\"Training Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f} | Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ba48a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
